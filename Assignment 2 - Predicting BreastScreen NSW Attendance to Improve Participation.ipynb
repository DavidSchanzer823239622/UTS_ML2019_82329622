{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting BreastScreen NSW Attendance to Improve Participation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to determine the accuracy with which attendance on next screening episode can be predicted, classifying women as either \"regular\" or \"lapsed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from joblib import dump, load\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "dtypes = {\n",
    "'CountryOfBirth':                           'category',\n",
    "'MainLanguage':                             'category',\n",
    "'IndigenousStatus':                         'category',\n",
    "'RemotenessArea':                           'category',\n",
    "'RelativeSocioEconomicDisadvantageDecile':  'category',\n",
    "'HistoryPreviousCancer':                    bool,\n",
    "'HistoryFamilyCancer':                      bool,\n",
    "'TotalEpisodes':                            int,\n",
    "'HasBeenDNA':                               bool,\n",
    "'HadAssessment':                            bool,\n",
    "'HadNeedleBiopsy':                          bool,\n",
    "'HadTechRecall':                            bool,\n",
    "'AgeAtMostRecentEpisode':                   int,\n",
    "'DistanceKms':                              float,\n",
    "'MonthMostRecentScreening':                 'category',\n",
    "'DayOfWeekMostRecentScreening':             'category',\n",
    "'HourOfDayMostRecentScreening':             'category',\n",
    "'VenueTypeMostRecentScreening':             'category',\n",
    "'FilmsTakenMostRecentScreening':            int,\n",
    "'DaysFromAttendanceToResultSent':           int,\n",
    "'LapsedRegular3rdMostRecentEpisode':        'category',\n",
    "'LapsedRegular2ndMostRecentEpisode':        'category',\n",
    "'LapsedRegularMostRecentEpisode':           'category'}\n",
    "\n",
    "df = pd.read_csv('Data extraction - 1000 observations scrambled.csv', dtype = dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(824812, 23)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many rows and columns?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountryOfBirth</th>\n",
       "      <th>MainLanguage</th>\n",
       "      <th>IndigenousStatus</th>\n",
       "      <th>RemotenessArea</th>\n",
       "      <th>RelativeSocioEconomicDisadvantageDecile</th>\n",
       "      <th>HistoryPreviousCancer</th>\n",
       "      <th>HistoryFamilyCancer</th>\n",
       "      <th>TotalEpisodes</th>\n",
       "      <th>HasBeenDNA</th>\n",
       "      <th>HadAssessment</th>\n",
       "      <th>...</th>\n",
       "      <th>DistanceKms</th>\n",
       "      <th>MonthMostRecentScreening</th>\n",
       "      <th>DayOfWeekMostRecentScreening</th>\n",
       "      <th>HourOfDayMostRecentScreening</th>\n",
       "      <th>VenueTypeMostRecentScreening</th>\n",
       "      <th>FilmsTakenMostRecentScreening</th>\n",
       "      <th>DaysFromAttendanceToResultSent</th>\n",
       "      <th>LapsedRegular3rdMostRecentEpisode</th>\n",
       "      <th>LapsedRegular2ndMostRecentEpisode</th>\n",
       "      <th>LapsedRegularMostRecentEpisode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>Other (please specify)</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Major Cities of Australia</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lapsed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>LEBANON</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Major Cities of Australia</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lapsed</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>English Only</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Major Cities of Australia</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>English Only</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Major Cities of Australia</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lapsed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>English Only</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Major Cities of Australia</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CountryOfBirth            MainLanguage IndigenousStatus  \\\n",
       "0          INDIA  Other (please specify)   Non-indigenous   \n",
       "1        LEBANON                  Arabic   Non-indigenous   \n",
       "2      AUSTRALIA            English Only   Non-indigenous   \n",
       "3      AUSTRALIA            English Only   Non-indigenous   \n",
       "4      AUSTRALIA            English Only   Non-indigenous   \n",
       "\n",
       "              RemotenessArea RelativeSocioEconomicDisadvantageDecile  \\\n",
       "0  Major Cities of Australia                                       3   \n",
       "1  Major Cities of Australia                                       3   \n",
       "2  Major Cities of Australia                                      10   \n",
       "3  Major Cities of Australia                                      10   \n",
       "4  Major Cities of Australia                                      10   \n",
       "\n",
       "   HistoryPreviousCancer  HistoryFamilyCancer  TotalEpisodes  HasBeenDNA  \\\n",
       "0                  False                False              2       False   \n",
       "1                  False                False              5        True   \n",
       "2                  False                 True              3       False   \n",
       "3                  False                False              2       False   \n",
       "4                  False                False              8       False   \n",
       "\n",
       "   HadAssessment  ...  DistanceKms  MonthMostRecentScreening  \\\n",
       "0          False  ...          0.0                        12   \n",
       "1          False  ...          0.0                         8   \n",
       "2          False  ...          0.0                        11   \n",
       "3          False  ...          4.0                         3   \n",
       "4          False  ...          2.0                        12   \n",
       "\n",
       "   DayOfWeekMostRecentScreening  HourOfDayMostRecentScreening  \\\n",
       "0                             5                             9   \n",
       "1                             4                             9   \n",
       "2                             3                            17   \n",
       "3                             3                             9   \n",
       "4                             4                             9   \n",
       "\n",
       "  VenueTypeMostRecentScreening FilmsTakenMostRecentScreening  \\\n",
       "0                        Fixed                             4   \n",
       "1                        Fixed                             4   \n",
       "2                        Fixed                             4   \n",
       "3                        Fixed                             4   \n",
       "4                        Fixed                             4   \n",
       "\n",
       "  DaysFromAttendanceToResultSent LapsedRegular3rdMostRecentEpisode  \\\n",
       "0                              4                               NaN   \n",
       "1                              6                               NaN   \n",
       "2                             14                               NaN   \n",
       "3                              6                               NaN   \n",
       "4                              5                           Regular   \n",
       "\n",
       "   LapsedRegular2ndMostRecentEpisode  LapsedRegularMostRecentEpisode  \n",
       "0                                NaN                          Lapsed  \n",
       "1                             Lapsed                         Regular  \n",
       "2                                NaN                         Regular  \n",
       "3                                NaN                          Lapsed  \n",
       "4                            Regular                         Regular  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountryOfBirth                             category\n",
       "MainLanguage                               category\n",
       "IndigenousStatus                           category\n",
       "RemotenessArea                             category\n",
       "RelativeSocioEconomicDisadvantageDecile    category\n",
       "HistoryPreviousCancer                          bool\n",
       "HistoryFamilyCancer                            bool\n",
       "TotalEpisodes                                 int64\n",
       "HasBeenDNA                                     bool\n",
       "HadAssessment                                  bool\n",
       "HadNeedleBiopsy                                bool\n",
       "HadTechRecall                                  bool\n",
       "AgeAtMostRecentEpisode                        int64\n",
       "DistanceKms                                 float64\n",
       "MonthMostRecentScreening                   category\n",
       "DayOfWeekMostRecentScreening               category\n",
       "HourOfDayMostRecentScreening               category\n",
       "VenueTypeMostRecentScreening               category\n",
       "FilmsTakenMostRecentScreening                 int64\n",
       "DaysFromAttendanceToResultSent                int64\n",
       "LapsedRegular3rdMostRecentEpisode          category\n",
       "LapsedRegular2ndMostRecentEpisode          category\n",
       "LapsedRegularMostRecentEpisode             category\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.7/site-packages/pandas_profiling/describe.py:392: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "  variable_stats = pd.concat(ldesc, join_axes=pd.Index([names]), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Generate pandas_profiling output for EDA\n",
    "pfr = pandas_profiling.ProfileReport(df)\n",
    "pfr.to_file(\"pandas-profiling output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HistoryPreviousCancer',\n",
       " 'HistoryFamilyCancer',\n",
       " 'TotalEpisodes',\n",
       " 'HasBeenDNA',\n",
       " 'HadAssessment',\n",
       " 'HadNeedleBiopsy',\n",
       " 'HadTechRecall',\n",
       " 'AgeAtMostRecentEpisode',\n",
       " 'DistanceKms',\n",
       " 'FilmsTakenMostRecentScreening',\n",
       " 'DaysFromAttendanceToResultSent',\n",
       " 'LapsedRegularMostRecentEpisode',\n",
       " 'CountryOfBirth_AFGHANISTAN',\n",
       " 'CountryOfBirth_ALBANIA',\n",
       " 'CountryOfBirth_ALGERIA',\n",
       " 'CountryOfBirth_ARGENTINA',\n",
       " 'CountryOfBirth_ARMENIA',\n",
       " 'CountryOfBirth_ARUBA',\n",
       " 'CountryOfBirth_AUSTRALIA',\n",
       " 'CountryOfBirth_AUSTRIA',\n",
       " 'CountryOfBirth_BANGLADESH',\n",
       " 'CountryOfBirth_BELGIUM',\n",
       " 'CountryOfBirth_BHUTAN',\n",
       " 'CountryOfBirth_BOLIVIA, PLURINATIONAL STATE OF',\n",
       " 'CountryOfBirth_BOSNIA AND HERZEGOVINA',\n",
       " 'CountryOfBirth_BRAZIL',\n",
       " 'CountryOfBirth_BRUNEI DARUSSALAM',\n",
       " 'CountryOfBirth_BULGARIA',\n",
       " 'CountryOfBirth_CAMBODIA',\n",
       " 'CountryOfBirth_CANADA',\n",
       " 'CountryOfBirth_CENTRAL AFRICAN REPUBLIC',\n",
       " 'CountryOfBirth_CHILE',\n",
       " 'CountryOfBirth_CHINA',\n",
       " 'CountryOfBirth_COCOS (KEELING) ISLANDS',\n",
       " 'CountryOfBirth_COLOMBIA',\n",
       " 'CountryOfBirth_CONGO',\n",
       " 'CountryOfBirth_CONGO, THE DEMOCRATIC REPUBLIC OF THE',\n",
       " 'CountryOfBirth_COOK ISLANDS',\n",
       " 'CountryOfBirth_COSTA RICA',\n",
       " 'CountryOfBirth_CROATIA',\n",
       " 'CountryOfBirth_CYPRUS',\n",
       " 'CountryOfBirth_CZECH REPUBLIC',\n",
       " 'CountryOfBirth_DENMARK',\n",
       " 'CountryOfBirth_ECUADOR',\n",
       " 'CountryOfBirth_EGYPT',\n",
       " 'CountryOfBirth_EL SALVADOR',\n",
       " 'CountryOfBirth_ESTONIA',\n",
       " 'CountryOfBirth_ETHIOPIA',\n",
       " 'CountryOfBirth_FIJI',\n",
       " 'CountryOfBirth_FINLAND',\n",
       " 'CountryOfBirth_FRANCE',\n",
       " 'CountryOfBirth_GEORGIA',\n",
       " 'CountryOfBirth_GERMANY',\n",
       " 'CountryOfBirth_GHANA',\n",
       " 'CountryOfBirth_GIBRALTAR',\n",
       " 'CountryOfBirth_GREECE',\n",
       " 'CountryOfBirth_GRENADA',\n",
       " 'CountryOfBirth_GUINEA',\n",
       " 'CountryOfBirth_HONDURAS',\n",
       " 'CountryOfBirth_HONG KONG',\n",
       " 'CountryOfBirth_HUNGARY',\n",
       " 'CountryOfBirth_INDIA',\n",
       " 'CountryOfBirth_INDONESIA',\n",
       " 'CountryOfBirth_IRAN',\n",
       " 'CountryOfBirth_IRAQ',\n",
       " 'CountryOfBirth_IRELAND',\n",
       " 'CountryOfBirth_ISLE OF MAN',\n",
       " 'CountryOfBirth_ISRAEL',\n",
       " 'CountryOfBirth_ITALY',\n",
       " 'CountryOfBirth_JAMAICA',\n",
       " 'CountryOfBirth_JAPAN',\n",
       " 'CountryOfBirth_JORDAN',\n",
       " 'CountryOfBirth_KENYA',\n",
       " 'CountryOfBirth_KIRIBATI',\n",
       " \"CountryOfBirth_KOREA, DEMOCRATIC PEOPLE'S REPUBLIC OF\",\n",
       " 'CountryOfBirth_KOREA, REPUBLIC OF',\n",
       " 'CountryOfBirth_KUWAIT',\n",
       " 'CountryOfBirth_KYRGYZSTAN',\n",
       " \"CountryOfBirth_LAO PEOPLE'S DEMOCRATIC REPUBLIC\",\n",
       " 'CountryOfBirth_LATVIA',\n",
       " 'CountryOfBirth_LEBANON',\n",
       " 'CountryOfBirth_LESOTHO',\n",
       " 'CountryOfBirth_LIBERIA',\n",
       " 'CountryOfBirth_LIBYAN ARAB JAMAHIRIYA',\n",
       " 'CountryOfBirth_LITHUANIA',\n",
       " 'CountryOfBirth_MACAO',\n",
       " 'CountryOfBirth_MACEDONIA, THE FORMER YUGOSLAV REPUBLIC OF',\n",
       " 'CountryOfBirth_MALAWI',\n",
       " 'CountryOfBirth_MALAYSIA',\n",
       " 'CountryOfBirth_MALTA',\n",
       " 'CountryOfBirth_MAURITIUS',\n",
       " 'CountryOfBirth_MEXICO',\n",
       " 'CountryOfBirth_MONTENEGRO',\n",
       " 'CountryOfBirth_MOROCCO',\n",
       " 'CountryOfBirth_MOZAMBIQUE',\n",
       " 'CountryOfBirth_MYANMAR',\n",
       " 'CountryOfBirth_NAMIBIA',\n",
       " 'CountryOfBirth_NEPAL',\n",
       " 'CountryOfBirth_NETHERLANDS',\n",
       " 'CountryOfBirth_NEW CALEDONIA',\n",
       " 'CountryOfBirth_NEW ZEALAND',\n",
       " 'CountryOfBirth_NICARAGUA',\n",
       " 'CountryOfBirth_NIGERIA',\n",
       " 'CountryOfBirth_NORFOLK ISLAND',\n",
       " 'CountryOfBirth_NORWAY',\n",
       " 'CountryOfBirth_PAKISTAN',\n",
       " 'CountryOfBirth_PALESTINIAN TERRITORY, OCCUPIED',\n",
       " 'CountryOfBirth_PANAMA',\n",
       " 'CountryOfBirth_PAPUA NEW GUINEA',\n",
       " 'CountryOfBirth_PARAGUAY',\n",
       " 'CountryOfBirth_PERU',\n",
       " 'CountryOfBirth_PHILIPPINES',\n",
       " 'CountryOfBirth_POLAND',\n",
       " 'CountryOfBirth_PORTUGAL',\n",
       " 'CountryOfBirth_ROMANIA',\n",
       " 'CountryOfBirth_RUSSIAN FEDERATION',\n",
       " 'CountryOfBirth_SAMOA',\n",
       " 'CountryOfBirth_SAUDI ARABIA',\n",
       " 'CountryOfBirth_SENEGAL',\n",
       " 'CountryOfBirth_SERBIA',\n",
       " 'CountryOfBirth_SEYCHELLES',\n",
       " 'CountryOfBirth_SINGAPORE',\n",
       " 'CountryOfBirth_SLOVAKIA',\n",
       " 'CountryOfBirth_SLOVENIA',\n",
       " 'CountryOfBirth_SOLOMON ISLANDS',\n",
       " 'CountryOfBirth_SOUTH AFRICA',\n",
       " 'CountryOfBirth_SPAIN',\n",
       " 'CountryOfBirth_SRI LANKA',\n",
       " 'CountryOfBirth_SUDAN',\n",
       " 'CountryOfBirth_SWEDEN',\n",
       " 'CountryOfBirth_SWITZERLAND',\n",
       " 'CountryOfBirth_SYRIAN ARAB REPUBLIC',\n",
       " 'CountryOfBirth_TAIWAN, PROVINCE OF CHINA',\n",
       " 'CountryOfBirth_TANZANIA, UNITED REPUBLIC OF',\n",
       " 'CountryOfBirth_THAILAND',\n",
       " 'CountryOfBirth_TIMOR-LESTE',\n",
       " 'CountryOfBirth_TONGA',\n",
       " 'CountryOfBirth_TRINIDAD AND TOBAGO',\n",
       " 'CountryOfBirth_TUNISIA',\n",
       " 'CountryOfBirth_TURKEY',\n",
       " 'CountryOfBirth_UGANDA',\n",
       " 'CountryOfBirth_UKRAINE',\n",
       " 'CountryOfBirth_UNITED KINGDOM',\n",
       " 'CountryOfBirth_UNITED STATES',\n",
       " 'CountryOfBirth_UNITED STATES MINOR OUTLYING ISLANDS',\n",
       " 'CountryOfBirth_URUGUAY',\n",
       " 'CountryOfBirth_Unknown',\n",
       " 'CountryOfBirth_VANUATU',\n",
       " 'CountryOfBirth_VENEZUELA, BOLIVARIAN REPUBLIC OF',\n",
       " 'CountryOfBirth_VIET NAM',\n",
       " 'CountryOfBirth_YEMEN',\n",
       " 'CountryOfBirth_ZIMBABWE',\n",
       " 'CountryOfBirth_ANGOLA',\n",
       " 'CountryOfBirth_BAHAMAS',\n",
       " 'CountryOfBirth_BAHRAIN',\n",
       " 'CountryOfBirth_BELARUS',\n",
       " 'CountryOfBirth_CHRISTMAS ISLAND',\n",
       " 'CountryOfBirth_CUBA',\n",
       " 'CountryOfBirth_DOMINICAN REPUBLIC',\n",
       " 'CountryOfBirth_FRENCH GUIANA',\n",
       " 'CountryOfBirth_GAMBIA',\n",
       " 'CountryOfBirth_GUYANA',\n",
       " 'CountryOfBirth_ICELAND',\n",
       " 'CountryOfBirth_JERSEY',\n",
       " 'CountryOfBirth_NIUE',\n",
       " 'CountryOfBirth_SIERRA LEONE',\n",
       " 'CountryOfBirth_SOMALIA',\n",
       " 'CountryOfBirth_SWAZILAND',\n",
       " 'CountryOfBirth_UZBEKISTAN',\n",
       " 'CountryOfBirth_WALLIS AND FUTUNA',\n",
       " 'CountryOfBirth_ZAMBIA',\n",
       " 'CountryOfBirth_AZERBAIJAN',\n",
       " 'CountryOfBirth_BARBADOS',\n",
       " 'CountryOfBirth_ERITREA',\n",
       " 'CountryOfBirth_FRENCH POLYNESIA',\n",
       " 'CountryOfBirth_GUERNSEY',\n",
       " 'CountryOfBirth_MOLDOVA, REPUBLIC OF',\n",
       " 'CountryOfBirth_NETHERLANDS ANTILLES',\n",
       " 'CountryOfBirth_QATAR',\n",
       " 'CountryOfBirth_BERMUDA',\n",
       " 'CountryOfBirth_GUATEMALA',\n",
       " 'CountryOfBirth_HAITI',\n",
       " 'CountryOfBirth_MONACO',\n",
       " 'CountryOfBirth_NAURU',\n",
       " 'CountryOfBirth_REUNION',\n",
       " 'CountryOfBirth_SURINAME',\n",
       " 'CountryOfBirth_UNITED ARAB EMIRATES',\n",
       " 'CountryOfBirth_DJIBOUTI',\n",
       " 'CountryOfBirth_GUAM',\n",
       " 'CountryOfBirth_KAZAKHSTAN',\n",
       " 'CountryOfBirth_LUXEMBOURG',\n",
       " 'CountryOfBirth_MONGOLIA',\n",
       " 'CountryOfBirth_SAINT HELENA',\n",
       " 'CountryOfBirth_SAO TOME AND PRINCIPE',\n",
       " 'CountryOfBirth_TOKELAU',\n",
       " 'CountryOfBirth_VIRGIN ISLANDS, U.S.',\n",
       " 'CountryOfBirth_BURUNDI',\n",
       " 'CountryOfBirth_MALDIVES',\n",
       " 'CountryOfBirth_MADAGASCAR',\n",
       " 'CountryOfBirth_CAMEROON',\n",
       " 'CountryOfBirth_PUERTO RICO',\n",
       " 'CountryOfBirth_SAINT KITTS AND NEVIS',\n",
       " 'CountryOfBirth_TOGO',\n",
       " 'CountryOfBirth_ALAND ISLANDS',\n",
       " 'CountryOfBirth_ANDORRA',\n",
       " 'CountryOfBirth_BOTSWANA',\n",
       " 'CountryOfBirth_TUVALU',\n",
       " 'CountryOfBirth_SOUTH GEORGIA AND THE SOUTH SANDWICH ISLANDS',\n",
       " 'CountryOfBirth_BELIZE',\n",
       " 'CountryOfBirth_FALKLAND ISLANDS (MALVINAS)',\n",
       " 'CountryOfBirth_MAURITANIA',\n",
       " 'CountryOfBirth_OMAN',\n",
       " 'CountryOfBirth_AMERICAN SAMOA',\n",
       " \"CountryOfBirth_COTE D'IVOIRE\",\n",
       " 'CountryOfBirth_GREENLAND',\n",
       " 'CountryOfBirth_BENIN',\n",
       " 'CountryOfBirth_GUINEA-BISSAU',\n",
       " 'CountryOfBirth_TAJIKISTAN',\n",
       " 'CountryOfBirth_VIRGIN ISLANDS, BRITISH',\n",
       " 'CountryOfBirth_RWANDA',\n",
       " 'CountryOfBirth_BRITISH INDIAN OCEAN TERRITORY',\n",
       " 'CountryOfBirth_FAROE ISLANDS',\n",
       " 'CountryOfBirth_MALI',\n",
       " 'CountryOfBirth_MICRONESIA, FEDERATED STATES OF',\n",
       " 'CountryOfBirth_COMOROS',\n",
       " 'CountryOfBirth_CHAD',\n",
       " 'CountryOfBirth_GUADELOUPE',\n",
       " 'MainLanguage_Arabic',\n",
       " 'MainLanguage_Cantonese',\n",
       " 'MainLanguage_Croatian',\n",
       " 'MainLanguage_English Only',\n",
       " 'MainLanguage_French',\n",
       " 'MainLanguage_German',\n",
       " 'MainLanguage_Greek',\n",
       " 'MainLanguage_Hindi',\n",
       " 'MainLanguage_Indonesian',\n",
       " 'MainLanguage_Italian',\n",
       " 'MainLanguage_Korean',\n",
       " 'MainLanguage_Macedonian',\n",
       " 'MainLanguage_Maltese',\n",
       " 'MainLanguage_Mandarin',\n",
       " 'MainLanguage_Other (please specify)',\n",
       " 'MainLanguage_Serbian',\n",
       " 'MainLanguage_Spanish',\n",
       " 'MainLanguage_Tagalog (Filipino)',\n",
       " 'MainLanguage_Tamil',\n",
       " 'MainLanguage_Turkish',\n",
       " 'MainLanguage_Unknown',\n",
       " 'MainLanguage_Vietnamese',\n",
       " 'IndigenousStatus_Aboriginal',\n",
       " 'IndigenousStatus_Aboriginal and Torres Strait Islander',\n",
       " 'IndigenousStatus_Declines to Respond',\n",
       " 'IndigenousStatus_Non-indigenous',\n",
       " 'IndigenousStatus_Not stated',\n",
       " 'IndigenousStatus_Torres Strait Islander',\n",
       " 'RemotenessArea_Inner Regional Australia',\n",
       " 'RemotenessArea_Major Cities of Australia',\n",
       " 'RemotenessArea_Outer Regional Australia',\n",
       " 'RemotenessArea_Remote Australia',\n",
       " 'RemotenessArea_Very Remote Australia',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_0',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_1',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_10',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_2',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_3',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_4',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_5',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_6',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_7',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_8',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_9',\n",
       " 'MonthMostRecentScreening_0',\n",
       " 'MonthMostRecentScreening_1',\n",
       " 'MonthMostRecentScreening_10',\n",
       " 'MonthMostRecentScreening_11',\n",
       " 'MonthMostRecentScreening_12',\n",
       " 'MonthMostRecentScreening_2',\n",
       " 'MonthMostRecentScreening_3',\n",
       " 'MonthMostRecentScreening_4',\n",
       " 'MonthMostRecentScreening_5',\n",
       " 'MonthMostRecentScreening_6',\n",
       " 'MonthMostRecentScreening_7',\n",
       " 'MonthMostRecentScreening_8',\n",
       " 'MonthMostRecentScreening_9',\n",
       " 'DayOfWeekMostRecentScreening_0',\n",
       " 'DayOfWeekMostRecentScreening_1',\n",
       " 'DayOfWeekMostRecentScreening_2',\n",
       " 'DayOfWeekMostRecentScreening_3',\n",
       " 'DayOfWeekMostRecentScreening_4',\n",
       " 'DayOfWeekMostRecentScreening_5',\n",
       " 'DayOfWeekMostRecentScreening_6',\n",
       " 'DayOfWeekMostRecentScreening_7',\n",
       " 'HourOfDayMostRecentScreening_0',\n",
       " 'HourOfDayMostRecentScreening_10',\n",
       " 'HourOfDayMostRecentScreening_11',\n",
       " 'HourOfDayMostRecentScreening_12',\n",
       " 'HourOfDayMostRecentScreening_13',\n",
       " 'HourOfDayMostRecentScreening_14',\n",
       " 'HourOfDayMostRecentScreening_15',\n",
       " 'HourOfDayMostRecentScreening_16',\n",
       " 'HourOfDayMostRecentScreening_17',\n",
       " 'HourOfDayMostRecentScreening_18',\n",
       " 'HourOfDayMostRecentScreening_19',\n",
       " 'HourOfDayMostRecentScreening_7',\n",
       " 'HourOfDayMostRecentScreening_8',\n",
       " 'HourOfDayMostRecentScreening_9',\n",
       " 'HourOfDayMostRecentScreening_6',\n",
       " 'HourOfDayMostRecentScreening_20',\n",
       " 'HourOfDayMostRecentScreening_2',\n",
       " 'HourOfDayMostRecentScreening_22',\n",
       " 'HourOfDayMostRecentScreening_3',\n",
       " 'HourOfDayMostRecentScreening_23',\n",
       " 'HourOfDayMostRecentScreening_1',\n",
       " 'HourOfDayMostRecentScreening_21',\n",
       " 'VenueTypeMostRecentScreening_Fixed',\n",
       " 'VenueTypeMostRecentScreening_Mobile',\n",
       " 'LapsedRegular3rdMostRecentEpisode_Lapsed',\n",
       " 'LapsedRegular3rdMostRecentEpisode_Regular',\n",
       " 'LapsedRegular2ndMostRecentEpisode_Lapsed',\n",
       " 'LapsedRegular2ndMostRecentEpisode_Regular']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to clean missing data, normalise numeric features or remove correlated features,\n",
    "# as boosted trees (which is what we will be using) are very robust to these potential data problems\n",
    "\n",
    "# Perform one-hot encoding of all categorical columns\n",
    "\n",
    "# CountryOfBirth                             \n",
    "one_hot = pd.get_dummies(df['CountryOfBirth'], prefix = 'CountryOfBirth')\n",
    "df = df.drop('CountryOfBirth',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# MainLanguage                             \n",
    "one_hot = pd.get_dummies(df['MainLanguage'], prefix = 'MainLanguage')\n",
    "df = df.drop('MainLanguage',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# IndigenousStatus                             \n",
    "one_hot = pd.get_dummies(df['IndigenousStatus'], prefix = 'IndigenousStatus')\n",
    "df = df.drop('IndigenousStatus',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# RemotenessArea                             \n",
    "one_hot = pd.get_dummies(df['RemotenessArea'], prefix = 'RemotenessArea')\n",
    "df = df.drop('RemotenessArea',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# RelativeSocioEconomicDisadvantageDecile                             \n",
    "one_hot = pd.get_dummies(df['RelativeSocioEconomicDisadvantageDecile'], prefix = 'RelativeSocioEconomicDisadvantageDecile')\n",
    "df = df.drop('RelativeSocioEconomicDisadvantageDecile',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# MonthMostRecentScreening                             \n",
    "one_hot = pd.get_dummies(df['MonthMostRecentScreening'], prefix = 'MonthMostRecentScreening')\n",
    "df = df.drop('MonthMostRecentScreening',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# DayOfWeekMostRecentScreening                             \n",
    "one_hot = pd.get_dummies(df['DayOfWeekMostRecentScreening'], prefix = 'DayOfWeekMostRecentScreening')\n",
    "df = df.drop('DayOfWeekMostRecentScreening',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# HourOfDayMostRecentScreening                             \n",
    "one_hot = pd.get_dummies(df['HourOfDayMostRecentScreening'], prefix = 'HourOfDayMostRecentScreening')\n",
    "df = df.drop('HourOfDayMostRecentScreening',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# VenueTypeMostRecentScreening                             \n",
    "one_hot = pd.get_dummies(df['VenueTypeMostRecentScreening'], prefix = 'VenueTypeMostRecentScreening')\n",
    "df = df.drop('VenueTypeMostRecentScreening',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# LapsedRegular3rdMostRecentEpisode                             \n",
    "one_hot = pd.get_dummies(df['LapsedRegular3rdMostRecentEpisode'], prefix = 'LapsedRegular3rdMostRecentEpisode')\n",
    "df = df.drop('LapsedRegular3rdMostRecentEpisode',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# LapsedRegular2ndMostRecentEpisode                             \n",
    "one_hot = pd.get_dummies(df['LapsedRegular2ndMostRecentEpisode'], prefix = 'LapsedRegular2ndMostRecentEpisode')\n",
    "df = df.drop('LapsedRegular2ndMostRecentEpisode',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(824812, 320)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(824812, 319)\n",
      "(824812,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into X and y\n",
    "TargetVariable = 'LapsedRegularMostRecentEpisode'\n",
    "X = df.loc[:, df.columns != TargetVariable]\n",
    "y = np.ravel(df.loc[:, df.columns == TargetVariable])\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the string target class values (\"regular\" or \"lapsed\") as integers in a numpy array\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(y)\n",
    "label_encoded_y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into 70% training, 30% test, using the encoded numpy array\n",
    "seed = 7\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "Untuned accuracy: 78.71%\n",
      "--- 787.0630960464478 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Attempt 1: call XGBClassifier with no parameters, using all of the default values, to measure the baseline accuracy\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model to training data\n",
    "model_baseline = xgb.XGBClassifier()\n",
    "model_baseline.fit(X_train, y_train)\n",
    "print(model_baseline)\n",
    "# Output:\n",
    "#     XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#                   colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "#                   learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "#                   min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
    "#                   nthread=None, objective='binary:logistic', random_state=0,\n",
    "#                   reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "#                   silent=None, subsample=1, verbosity=1)\n",
    "\n",
    "# Make predictions for the test data\n",
    "y_pred = model_baseline.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate the predictions made using the test data\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Untuned accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "# Output:\n",
    "#     Untuned accuracy: 78.71%\n",
    "#     --- 787.0630960464478 seconds ---\n",
    "# So, our baseline accuracy on the test dataset is 78.71% - can we do better?\n",
    "\n",
    "# Save the model to disk using joblibâ€™s replacement of pickle (dump & load), which is more efficient on objects\n",
    "# that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators\n",
    "dump(model_baseline, 'model_baseline.joblib')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuned cross-validated accuracy: 78.68%\n",
      "Untuned cross-validated AUC: 0.7880\n",
      "--- 360.53999495506287 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_cv.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt 2: See if we can improve this using XGBoost's built-in cross-validation capabilities\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the DMatrix required for the xgboost cv method, using some minimal parameters\n",
    "dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
    "params={\"objective\":\"binary:logistic\",\"max_depth\":4}\n",
    "model_cv = xgb.cv(dtrain=dmatrix, params=params, nfold=4, num_boost_round=10, metrics=[\"error\",\"auc\"],\n",
    "                    as_pandas=True)\n",
    "\n",
    "print(\"Untuned cross-validated accuracy: %.2f%%\" %(((1 - model_cv[\"test-error-mean\"]).iloc[-1]) * 100.0))\n",
    "print(\"Untuned cross-validated AUC: %.4f\" %(((model_cv[\"test-auc-mean\"]).iloc[-1])))\n",
    "# Output:\n",
    "#     Untuned cross-validated accuracy: 78.68%\n",
    "# which is slightly worse than the baseline accuracy of 78.71%\n",
    "#     Untuned cross-validated AUC: 0.7880\n",
    "#     --- 360.53999495506287 seconds ---\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_cv, 'model_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated early_stopping accuracy: 79.42%\n",
      "Cross-validated early_stopping AUC: 0.8010\n",
      "--- 1507.6005401611328 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_cv_early_stopping.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt 3: See if we can improve this using a higher number of boosting rounds\n",
    "# with automated boosting round selection using early_stopping\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the DMatrix required for the xgboost cv method, using some minimal parameters\n",
    "dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
    "params={\"objective\":\"binary:logistic\",\"max_depth\":4}\n",
    "model_cv_early_stopping = xgb.cv(dtrain=dmatrix, params=params, early_stopping_rounds=10, nfold=4,\n",
    "                                   num_boost_round=50, metrics=[\"error\",\"auc\"], seed=0, as_pandas=True)\n",
    "\n",
    "print(\"Cross-validated early_stopping accuracy: %.2f%%\" %(((1 - model_cv_early_stopping[\"test-error-mean\"]).iloc[-1]) * 100.0))\n",
    "print(\"Cross-validated early_stopping AUC: %.4f\" %(((model_cv_early_stopping[\"test-auc-mean\"]).iloc[-1])))\n",
    "# Output:\n",
    "#     Cross-validated early_stopping accuracy: 79.42%\n",
    "# which is slightly higher than the baseline accuracy of 78.71%\n",
    "#     Cross-validated early_stopping AUC: 0.8010\n",
    "#--- 1507.6005401611328 seconds ---\n",
    "# which is slightly higher than the untuned cross-validated AUC of 0.7880\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_cv_early_stopping, 'model_cv_early_stopping.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some frequently tuned XGBoost parameters for the tree base learner:\n",
    "#\n",
    "# learning rate (eta):\n",
    "#     Affects how quickly the model fits the residual error using additional base learners.\n",
    "#     A low learning rate will require more boosting rounds to achieve the same reduction in residual error as an\n",
    "#     XGBoost model with a high learning rate.\n",
    "# gamma:\n",
    "#     Minimum loss reduction to create new tree split, which affects how strongly regularised the trained model will\n",
    "#     be.\n",
    "# lambda:\n",
    "#     L2 regularisation on leaf weights, which affects how strongly regularised the trained model will be.\n",
    "# alpha:\n",
    "#     L1 regularisation on leaf weights, which affects how strongly regularised the trained model will be.\n",
    "# max_depth:\n",
    "#     Maximum depth per tree.\n",
    "#     Must be a positive integer value.\n",
    "#     Affects how deeply each tree is allowed to grow during any given boosting round.\n",
    "# subsample:\n",
    "#     Percentage of samples used per tree.\n",
    "#     Must be a value between 0 and 1.\n",
    "#     The fraction of the total training set that can be used for any given boosting round.\n",
    "#     If the value is low, then the fraction of training data used per boosting round would be low and may run into\n",
    "#     underfitting problems, while a value that is very high can lead to overfitting.\n",
    "# colsample_bytree:\n",
    "#     Percentage of features used per tree.\n",
    "#     The fraction of features that can be selected from during any given boosting round.\n",
    "#     Must be a value between 0 and 1.\n",
    "#     A large value means that almost all features can be used to build a tree during a given boosting round,\n",
    "#     while a small value means that the fraction of features that can be selected from is very small.\n",
    "#     Smaller values can be thought of as providing additional regularisation to the model,\n",
    "#     while using all columns may overfit a trained model.\n",
    "# num_boost_round:\n",
    "#     Number of boosting rounds.\n",
    "#     The number of trees to be built or the number of base learners to be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 25.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n",
      "Best AUC found: %.4f 0.7997267093920994\n",
      "--- 1969.9332740306854 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_grid_search.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt 4: See if we can improve this by tuning a few of the above hyperparameters using GridSearch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    \"colsample_bytree\":[0.3, 0.7],\n",
    "    \"n_estimators\": [50],\n",
    "    \"max_depth\": [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the classifier\n",
    "gbm = xgb.XGBClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "model_grid_search = GridSearchCV(param_grid=gbm_param_grid, estimator=gbm, scoring=\"roc_auc\", cv=4,\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit model_grid_search to the data\n",
    "model_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", model_grid_search.best_params_)\n",
    "print(\"Best AUC found: %.4f\", %(model_grid_search.best_score_))\n",
    "\n",
    "# Output:\n",
    "#     Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
    "#     [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "#     [Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed: 25.3min finished\n",
    "#     Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n",
    "#     Best AUC found:  0.7997\n",
    "#     --- 1969.9332740306854 seconds ---\n",
    "# This AUC is slightly worse than the cross-validated early_stopping AUC of 0.8010\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_grid_search, 'model_grid_search.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create XGBoost models and perform cross-validation\n",
    "def modelfit(alg, data, label, useTrainCV = True, cv_folds = 5, early_stopping_rounds = 50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(data=data, label=label)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    # Fit the algorithm on the data\n",
    "    alg.fit(data, label, eval_metric='auc', verbose=True)\n",
    "        \n",
    "    # Predict training set\n",
    "    dtrain_predictions = alg.predict(data)\n",
    "    dtrain_predprob = alg.predict_proba(data)[:,1]\n",
    "        \n",
    "    # Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(label, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(label, dtrain_predprob))\n",
    "    print(\"Best Iteration: {}\".format(alg.get_booster().best_iteration))  \n",
    "    print(\"Best ntree limit: {}\".format(alg.get_booster().best_ntree_limit))\n",
    "    \n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore()).nlargest(50).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Top 50 Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8069\n",
      "AUC Score (Train): 0.820695\n",
      "--- 12947.70002579689 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step.joblib']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attempt 5: Try a step-by-step approach to hyperparameter tuning\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Fix the learning rate and the number of estimators\n",
    "model_step_by_step = xgb.XGBClassifier(\n",
    "    learning_rate = 0.1,\n",
    "    n_estimators = 1000,\n",
    "    max_depth = 5,\n",
    "    min_child_weight = 1,\n",
    "    gamma = 0,\n",
    "    subsample = 0.8,\n",
    "    colsample_bytree = 0.8,\n",
    "    objective = 'binary:logistic',\n",
    "    nthread = 4,\n",
    "    scale_pos_weight = 1,\n",
    "    seed = 27)\n",
    "\n",
    "modelfit(model_step_by_step, X_train, y_train)\n",
    "\n",
    "# Save the Feature Importances bar chart to a file\n",
    "plt.savefig('Feature Importances - initial.png')\n",
    "\n",
    "# Output:\n",
    "#     Model Report\n",
    "#     Accuracy : 0.8069\n",
    "#     AUC Score (Train): 0.820695\n",
    "#     Best Iteration: 787\n",
    "#     Best ntree limit: 788\n",
    "#     --- 12947.70002579689 seconds ---\n",
    "# So, we've improved the AUC from 0.8010 to 0.820695. Let's keep tuning to see if we can improve this.\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step, 'model_step_by_step.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tune max_depth and min_child_weight, as they will have the highest impact on model outcome.\n",
    "# Use the optimal n_estimators value calculated in the previous step (788).\n",
    "# To start with, set wider ranges for max_depth and min_child_weight and then perform another iteration for\n",
    "# smaller ranges.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "param_step_by_step_test1 = {\n",
    "    'max_depth': range(3, 10, 2),\n",
    "    'min_child_weight': range(1, 6, 2)\n",
    "}\n",
    "gsearch_step_by_step_test1 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate=0.1, n_estimators = 788,\n",
    "                                          max_depth = 5, min_child_weight = 1, gamma = 0, subsample = 0.8,\n",
    "                                          colsample_bytree = 0.8, objective = 'binary:logistic', nthread = 4,\n",
    "                                          scale_pos_weight = 1, seed = 27), param_grid = param_step_by_step_test1,\n",
    "                                          scoring = 'roc_auc',n_jobs = 4, iid = False, cv = 5)\n",
    "gsearch_step_by_step_test1.fit(X_train, y_train)\n",
    "print(gsearch_step_by_step_test1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "\n",
    "# Output:\n",
    "#     --- ?????? seconds ---\n",
    "# So, \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_test1, 'model_step_by_step_test1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
