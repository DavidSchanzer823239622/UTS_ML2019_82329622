{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting BreastScreen NSW Attendance to Improve Participation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by David Schanzer, Student ID 82329622"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Link to Notebook in GitHub repo:\n",
    "https://github.com/DavidSchanzer823239622/UTS_ML2019_82329622/blob/master/Assignment%202%20-%20Predicting%20BreastScreen%20NSW%20Attendance%20to%20Improve%20Participation%20-%20100000%20observations.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to determine the accuracy with which the attendance of women at their next breast screening appointment at a BreastScreen NSW clinic or mobile van can be predicted, classifying women as either \"regular\" (predicted to attend within 90 days of the rescreen date) or \"lapsed\" (predicted to not attend within 90 days of their rescreen date)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance of Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a sad fact that breast cancer is the most common cancer affecting women in NSW, with 1 in 8 NSW women developing breast cancer in their lifetime.\n",
    "\n",
    "Having a regular screening mammogram is the most effective way to find breast cancer early in women aged over 50, because the best time to treat breast cancer is when it is still very small and has not spread. As a result, attending a breast screen appointment every two years is vital to reduce deaths from breast cancer.\n",
    "\n",
    "Over 80% of women in NSW in the 50-69 age range have at some point in their lives been screened by BreastScreen NSW. However, only 63% of those women screened within the last 24 months, with the remaining 37% being either under-screened, or having lapsed from the program completely. Rescreen rates are therefore a major driver of screening activity and participation rates.\n",
    "\n",
    "Understanding rescreening behaviour is necessary for the development of effective strategies for retaining women in the program, and thereby maintaining and improving participation rates. Being able to identify women at high risk of not rescreening will facilitate individual-level interventions (eg. a reminder phone call) to encourage regular rescreening.\n",
    "\n",
    "Therefore, this machine learning project aims to:\n",
    "1. Identify key predictors of non-reattendance at BreastScreen NSW\n",
    "2. Determine whether it is feasible to develop a prediction tool to flag individuals that are likely to lapse from the screening program, that is, not attend within 90 days of their next scheduled rescreen date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first challenge is to identify whether it is feasible to create a high-quality labelled data set that could be used for training a machine learning model. After discussion with relevant Cancer Institute NSW staff members, the BreastScreen data mart in the production data warehouse (database DATA_MART_SCREENING on server CISQL03PRD) was identified as a suitable, relatively clean data set in a dimensional data model.\n",
    "\n",
    "The next challenge was to identify suitable features (data attributes) that could either be directly extracted or derived from this data mart. After further discussion with the data custodian, a suitable labelled data set was identified and formal approval granted for the study to proceed in the form of a data request. A relatively complex SQL query was then developed, and this has been included in Appendix 1. In total, 824,812 observations were available, with 23 attributes including the target attribute. These are detailed in the next section.\n",
    "\n",
    "Next, it was necessary to select a suitable machine learning algorithm. After discussion with the tutor of my Machine Learning subject, the XGBoost gradient boosting library was selected, due to its well-regarded reputation for being highly efficient and flexible, its popularity and success in relevant classification tasks in Kaggle competitions, and the availability of a Python implementation with a wealth of available information on how best to implement it.\n",
    "\n",
    "The next challenge was to select a suitable execution platform for the building and testing of the model. The approach taken was to undertake the work using an Anaconda Python distribution using Jupyter Notebooks on an available and powerful MacBook. This decision was made partly because utilising the alternative available platform, Google Colab, would have required uploading production (although deidentified) data, and partly because the Google Colab platform, while offering the use of high-performance GPUs, has a low limit on the amount of memory made available for free use, and the compute platforms are often busy and therefore unavailable.\n",
    "\n",
    "My complete lack of experience with XGBoost was the next challenge to be overcome, which was met with a combination of DataCamp online training, online searching, and advice from both my Machine Learning subject lecturer and tutors.\n",
    "\n",
    "The final challenge was, ironically, the amount of data available for analysis. Given that my proposed approach involved a large amount of hyperparameter tuning, my initial approach of using all 824,812 observations proved to be impractical due to excessive execution times, and so I made the decision to proceed with only 100,000 randomly-chosen observations for the purpose of this assignment, in the hope that these would be a suitable representative subsample of the entire set, and therefore be a suitable model for predicting unseen values for business use.\n",
    "\n",
    "***Note that, because of the sensitivity of the data, the GitHub folder (https://github.com/DavidSchanzer823239622/UTS_ML2019_82329622) contains only sample data that will not receive the same results obtained with the 100,000 observations when run on Google Colab; these are 1000 observations only, with the data values scrambled.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan for Data Models and Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 23 features of the data set, with some preliminary data analysis, are as follows:\n",
    "\n",
    "| No. | Name                   | Distinct values | Distribution                                                     |\n",
    "| --: | :--------------------- | :-------------: | :-----------------------------------------------------------     |\n",
    "|   1 | Country of birth       |       215       | 65.8% = 'AUSTRALIA', 5.9% = 'UNITED KINGDOM', 2.4% = 'CHINA',... |\n",
    "|   2 | Main language spoken   |        22       | 81.3% = 'English Only', 3.8% = 'Other (please specify)', 2.4% = 'Cantonese',... |\n",
    "|   3 | Indigenous status      |         6       | 98.4% = 'Non-indigenous', 1.2% = 'Aboriginal', 0.27% = 'Not stated', 0.06% = 'Aboriginal and Torres Strait Islander', 0.03% = 'Torres Strait Islander', 0.02% = 'Declines to Respond'\n",
    "|   4 | Remoteness Area        |         6       | 0.97% = NULL, then 69.7% = 'Major Cities of Australia', 23.6% = 'Inner Regional Australia', 5.4% = 'Outer Regional Australia', 0.33% = 'Remote Australia', 0.07% = 'Very Remote Australia'\n",
    "|   5 | Relative Socio-Economic Disadvantage Decile | 11 | 2.6% = NULL, then 28.1% = 10, 17.8% = 6, 9.8% = 3, 9.5% = 5,... |\n",
    "|   6 | Does the woman have any history of previous cancer external to program? | 2 | 98.6% = 0, 1.4% = 1 |\n",
    "|   7 | Does the woman have any family history of cancer? | 2  | 74.6% = 0, 25.4% = 1 |\n",
    "|   8 | Total number of episodes (previous BreastScreen encounters) | N/A | min = 2, max = 29, avg = 6, stdev = 3.8 **(binned)** |\n",
    "|   9 | Has the woman been \"DNA\" (did not attend) at any point in her past as part of the program? | 2 | 72.7% = 0, 27.3% = 1 |\n",
    "|  10 | Has the woman had an assessment (ie. asked to come back for a more diagnostic test due to something suspicious on her mammogram) at any point in her past as part of the program? | 2 | 74.6% = 0, 25.4% = 1 |\n",
    "|  11 | Has the woman had a needle biopsy (a potentially painful or traumatic procedure) at any point in her past as part of the program? | 2 | 93.4% = 0, 6.6% = 1 |\n",
    "|  12 | Did the woman have a Technical Recall (asked to return for another mammogram due to an inadequate image) at any point in her past as part of the program? | 2 | 96.9% = 0, 3.1% = 1 |\n",
    "|  13 | Age at most recent episode | N/A | 0% = NULL, min = 40, max = 109, avg = 63, stdev = 8.3 **(binned)** |\n",
    "|  14 | Distance from residential address to location of most recent episode | N/A | 1.23% = NULL, min = 0, max = 3860, avg = 15.6, stdev = 82.6 **(binned)** |\n",
    "|  15 | Month of year of most recent screening attendance | 13 | 0.006% = NULL, 10.0% = 8 (May), ..., 4.8% = 12 (Dec) |\n",
    "|  16 | Day of week of most recent screening attendance | 8 | 0.006% = NULL, 21.8% = 4 (Wed), ..., 0.54% = 1 (Sun) |\n",
    "|  17 | Hour of day of most recent screening attendance | 25 | 0.006% = NULL, 15.0% = 11 (11am-12pm), 14.3% = 9 (9am-10pm), ..., 0.0001% = 21 (9-10pm)\n",
    "|  18 | Type of venue of most recent screening attendance | 3 | 11.3% = NULL, 66.7% = 'Fixed', 22.0% = 'Mobile' |\n",
    "|  19 | Number of films (x-rays) taken at most recent screening attendance | N/A | 0.07% = NULL, min = 0, max = 54, avg = 4, stdev = 0.87 **(binned)** |\n",
    "|  20 | How many days did the woman have to wait for results after her most recent attendance? | N/A | 0.82% = NULL, min = 0, max = 100, avg = 6, stdev = 4.6 **(binned)** |\n",
    "|  21 | Was the woman 'regular' or 'lapsed' at her 3rd most recent episode? | 3 | 37.4% = NULL, 49.8% = 'Regular', 12.9% = 'Lapsed' |\n",
    "|  22 | Was the woman 'regular' or 'lapsed' at her 2nd most recent episode? | 3 | 21.7% = NULL, 61.0% = 'Regular', 17.4% = 'Lapsed' |\n",
    "|  23 | ***TARGET VARIABLE:*** Was the woman 'regular' or 'lapsed' at her most recent episode? | 2 | 74.4% = 'Regular', 25.6% = 'Lapsed' |\n",
    "\n",
    "Data was acquired by directly querying the data mart database identified in the previous section, using the SQL statements in Appendix 1.\n",
    "\n",
    "Data quality control was undertaken through exploratory data analysis, using both SQL (which yielded modifications to the SQL code, as well as clarifications about individual data items from the screening data subject matter expert) and the pandas_profiling Python library (which yielded the report [here](http://htmlpreview.github.com/?https://github.com/DavidSchanzer823239622/UTS_ML2019_82329622/blob/master/pandas-profiling%20output.html \"Output from pandas_profiling\")). The pandas_profiling report revealed no particular data problems that needed addressing.\n",
    "\n",
    "In considering different machine learning modelling techniques, it was discussed above that XGBoost was chosen due to:\n",
    "* it being recommended for the task by the tutor of my Machine Learning subject\n",
    "* its reputation for being designed to be highly efficient and flexible\n",
    "* its popularity and success in relevant classification tasks in Kaggle competitions\n",
    "* the availability of a Python implementation with a wealth of available information on how best to implement it\n",
    "\n",
    "One additional consideration in favour of XGBoost not mentioned above was the availability of a Feature Importance ranking through use of the get_booster().get_fscore() methods - this would allow me to determine which of the Top N input attributes had the largest impact on the predicted target variable. I felt this would add business value to my investigation.\n",
    "\n",
    "Consideration was given to also utilising a Random Forest classifier for comparison, as it is also rated highly in terms of its prediction accuracy. However, it rates poorly in terms of interpretability, as it lacks an equivalent Feature Importance rank to the XGBoost ranking. As a result, this classifier was excluded.\n",
    "\n",
    "The evaluation method chosen to measure the success of the investigation was to set aside 30% of the available data as a Test Set, which would be used only at the beginning to get a baseline accuracy, and at the end to evaluate the effects of tuning on the accuracy. All hyperparameter tuning between these two points would be done exclusively using the Training Set (the remaining 70%) using various subsets as Validation Sets. In tuning the hyperparameters, the AUC measure (area under the receiver operating characteristics curve) would be used as the value to be maximised, as this is appropriate to binary classification problems.\n",
    "\n",
    "The criteria for success would ultimately be determined by the BreastScreen business team, based on whether the achieved accuracy would be sufficient for the model to be deployed into production in order to make predictions of non-attendance so that interventions could take place. For the purpose of my investigation, my aim was to improve upon the baseline accuracy through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental methodology undertaken was as follows:\n",
    "1. Import data\n",
    "2. Perform exploratory data analysis to check for completeness, quality and other data issues\n",
    "3. Bin the numeric variables, to aid interpretability of the Feature Importance chart.\n",
    "4. Perform one-hot encoding of all categorical columns\n",
    "5. Split the data into X and y\n",
    "6. Encode the string target class values (\"regular\" or \"lapsed\") as integers in a numpy array\n",
    "7. Split the data set into 70% training, 30% test, using the encoded numpy array\n",
    "8. **Approach 1**: call XGBClassifier with no parameters, using all of the default values, to measure the baseline accuracy\n",
    "9. **Approach 2**: See if we can improve this using XGBoost's built-in cross-validation capabilities\n",
    "10. **Approach 3**: See if we can improve this using a higher number of boosting rounds with automated boosting round selection using early_stopping\n",
    "11. **Approach 4**: See if we can improve this by tuning a few of the above hyperparameters using a very simple GridSearch\n",
    "\n",
    "**Approach 5**: Try a step-by-step approach to hyperparameter tuning:\n",
    "12. Step 1: Fix the learning rate and the number of estimators, with typical values for other parameters\n",
    "13. Step 2: Tune max_depth and min_child_weight, as they will have the highest impact on model outcome, using the optimal n_estimators value calculated in the previous step. To start with, set wider ranges for max_depth and min_child_weight and then perform another iteration for smaller ranges.\n",
    "14. Step 3: Fine-tune max_depth and min_child_weight, looking for optimum values, by searching for values 1 above and below the best values discovered so far.\n",
    "15. Step 4: Tune gamma using the parameters already tuned above.\n",
    "16. Step 5: Re-calibrate the number of boosting rounds for the updated parameters\n",
    "17. Step 6: Tune subsample and colsample_bytree\n",
    "18. Step 7: Try values in 0.05 intervals around the best value so far for subsample and colsample_bytree\n",
    "19. Step 8: Apply regularization to reduce overfitting, by tuning reg_alpha\n",
    "20. Step 9: Try values of reg_alpha closer to the best value so far to see if we improve AUC\n",
    "21. Step 10: Apply this regularization (reg_alpha) in the model and look at the impact\n",
    "22. Step 11: Finally, lower the learning rate and add more trees, using the cv function of XGBoost\n",
    "\n",
    "\n",
    "23. Finally, evaluation: call XGBClassifier with the optimised parameters to measure the final achieved accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training of Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from joblib import dump, load\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "dtypes = {\n",
    "'CountryOfBirth':                           'category',    # Country of birth\n",
    "'MainLanguage':                             'category',    # Main language spoken\n",
    "'IndigenousStatus':                         'category',    # Indigenous status\n",
    "'RemotenessArea':                           'category',    # Remoteness Area\n",
    "'RelativeSocioEconomicDisadvantageDecile':  'category',    # Relative Socio-Economic Disadvantage Decile\n",
    "'HistoryPreviousCancer':                    bool,          # Does the woman have any history of previous cancer external to program?\n",
    "'HistoryFamilyCancer':                      bool,          # Does the woman have any family history of cancer?\n",
    "'TotalEpisodes':                            int,           # Total number of episodes (previous BreastScreen encounters)\n",
    "'HasBeenDNA':                               bool,          # Has the woman been \"DNA\" (did not attend) at any point in her past as part of the program?\n",
    "'HadAssessment':                            bool,          # Has the woman had an assessment (ie. asked to come back for a more diagnostic test due to something suspicious on her mammogram) at any point in her past as part of the program?\n",
    "'HadNeedleBiopsy':                          bool,          # Has the woman had a needle biopsy (a potentially painful or traumatic procedure) at any point in her past as part of the program?\n",
    "'HadTechRecall':                            bool,          # Did the woman have a Technical Recall (asked to return for another mammogram due to an inadequate image) at any point in her past as part of the program?\n",
    "'AgeAtMostRecentEpisode':                   int,           # Age at most recent episode\n",
    "'DistanceKms':                              float,         # Distance from residential address to location of most recent episode\n",
    "'MonthMostRecentScreening':                 'category',    # Month of year of most recent screening attendance\n",
    "'DayOfWeekMostRecentScreening':             'category',    # Day of week of most recent screening attendance\n",
    "'HourOfDayMostRecentScreening':             'category',    # Hour of day of most recent screening attendance\n",
    "'VenueTypeMostRecentScreening':             'category',    # Type of venue of most recent screening attendance\n",
    "'FilmsTakenMostRecentScreening':            int,           # Number of films (x-rays) taken at most recent screening attendance\n",
    "'DaysFromAttendanceToResultSent':           int,           # How many days did the woman have to wait for results after her most recent attendance?\n",
    "'LapsedRegular3rdMostRecentEpisode':        'category',    # Was the woman 'regular' or 'lapsed' at her 3rd most recent episode?\n",
    "'LapsedRegular2ndMostRecentEpisode':        'category',    # Was the woman 'regular' or 'lapsed' at her 2nd most recent episode?\n",
    "'LapsedRegularMostRecentEpisode':           'category'}    # TARGET VARIABLE: Was the woman 'regular' or 'lapsed' at her most recent episode?\n",
    "\n",
    "# Data import for the full data set has been commented out for Google Colab use\n",
    "#df_full = pd.read_csv('Data extraction.csv', dtype = dtypes)\n",
    "\n",
    "# Random sampling - Random 100,000 rows\n",
    "#df = df_full.sample(n = 100000)\n",
    "\n",
    "# For Google Colab:\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/DavidSchanzer823239622/UTS_ML2019_82329622/master/Data%20extraction%20-%201000%20observations%20scrambled.csv', dtype = dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 23)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many rows and columns?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountryOfBirth</th>\n",
       "      <th>MainLanguage</th>\n",
       "      <th>IndigenousStatus</th>\n",
       "      <th>RemotenessArea</th>\n",
       "      <th>RelativeSocioEconomicDisadvantageDecile</th>\n",
       "      <th>HistoryPreviousCancer</th>\n",
       "      <th>HistoryFamilyCancer</th>\n",
       "      <th>TotalEpisodes</th>\n",
       "      <th>HasBeenDNA</th>\n",
       "      <th>HadAssessment</th>\n",
       "      <th>...</th>\n",
       "      <th>DistanceKms</th>\n",
       "      <th>MonthMostRecentScreening</th>\n",
       "      <th>DayOfWeekMostRecentScreening</th>\n",
       "      <th>HourOfDayMostRecentScreening</th>\n",
       "      <th>VenueTypeMostRecentScreening</th>\n",
       "      <th>FilmsTakenMostRecentScreening</th>\n",
       "      <th>DaysFromAttendanceToResultSent</th>\n",
       "      <th>LapsedRegular3rdMostRecentEpisode</th>\n",
       "      <th>LapsedRegular2ndMostRecentEpisode</th>\n",
       "      <th>LapsedRegularMostRecentEpisode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>330518</td>\n",
       "      <td>NEW ZEALAND</td>\n",
       "      <td>English Only</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Major Cities of Australia</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786429</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>English Only</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Inner Regional Australia</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lapsed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625728</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>English Only</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Major Cities of Australia</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Lapsed</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Lapsed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721374</td>\n",
       "      <td>NEW ZEALAND</td>\n",
       "      <td>English Only</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Major Cities of Australia</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213098</td>\n",
       "      <td>ITALY</td>\n",
       "      <td>English Only</td>\n",
       "      <td>Non-indigenous</td>\n",
       "      <td>Major Cities of Australia</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CountryOfBirth  MainLanguage IndigenousStatus  \\\n",
       "330518    NEW ZEALAND  English Only   Non-indigenous   \n",
       "786429      AUSTRALIA  English Only   Non-indigenous   \n",
       "625728      AUSTRALIA  English Only   Non-indigenous   \n",
       "721374    NEW ZEALAND  English Only   Non-indigenous   \n",
       "213098          ITALY  English Only   Non-indigenous   \n",
       "\n",
       "                   RemotenessArea RelativeSocioEconomicDisadvantageDecile  \\\n",
       "330518  Major Cities of Australia                                      10   \n",
       "786429   Inner Regional Australia                                       5   \n",
       "625728  Major Cities of Australia                                       7   \n",
       "721374  Major Cities of Australia                                       7   \n",
       "213098  Major Cities of Australia                                       0   \n",
       "\n",
       "        HistoryPreviousCancer  HistoryFamilyCancer  TotalEpisodes  HasBeenDNA  \\\n",
       "330518                  False                False              2        True   \n",
       "786429                  False                False              4        True   \n",
       "625728                  False                False              9        True   \n",
       "721374                  False                False              2        True   \n",
       "213098                  False                False              2       False   \n",
       "\n",
       "        HadAssessment  ...  DistanceKms  MonthMostRecentScreening  \\\n",
       "330518          False  ...         11.0                         4   \n",
       "786429          False  ...         40.0                         6   \n",
       "625728           True  ...         16.0                         4   \n",
       "721374          False  ...          9.0                        11   \n",
       "213098          False  ...          4.0                        11   \n",
       "\n",
       "        DayOfWeekMostRecentScreening  HourOfDayMostRecentScreening  \\\n",
       "330518                             5                            13   \n",
       "786429                             6                             9   \n",
       "625728                             4                             9   \n",
       "721374                             5                            14   \n",
       "213098                             3                            10   \n",
       "\n",
       "       VenueTypeMostRecentScreening FilmsTakenMostRecentScreening  \\\n",
       "330518                        Fixed                             6   \n",
       "786429                        Fixed                             4   \n",
       "625728                        Fixed                             4   \n",
       "721374                        Fixed                             5   \n",
       "213098                        Fixed                             4   \n",
       "\n",
       "       DaysFromAttendanceToResultSent LapsedRegular3rdMostRecentEpisode  \\\n",
       "330518                              7                               NaN   \n",
       "786429                              5                               NaN   \n",
       "625728                              1                            Lapsed   \n",
       "721374                              5                               NaN   \n",
       "213098                              3                               NaN   \n",
       "\n",
       "        LapsedRegular2ndMostRecentEpisode  LapsedRegularMostRecentEpisode  \n",
       "330518                                NaN                         Regular  \n",
       "786429                                NaN                          Lapsed  \n",
       "625728                            Regular                          Lapsed  \n",
       "721374                                NaN                         Regular  \n",
       "213098                                NaN                         Regular  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountryOfBirth                             category\n",
       "MainLanguage                               category\n",
       "IndigenousStatus                           category\n",
       "RemotenessArea                             category\n",
       "RelativeSocioEconomicDisadvantageDecile    category\n",
       "HistoryPreviousCancer                          bool\n",
       "HistoryFamilyCancer                            bool\n",
       "TotalEpisodes                                 int64\n",
       "HasBeenDNA                                     bool\n",
       "HadAssessment                                  bool\n",
       "HadNeedleBiopsy                                bool\n",
       "HadTechRecall                                  bool\n",
       "AgeAtMostRecentEpisode                        int64\n",
       "DistanceKms                                 float64\n",
       "MonthMostRecentScreening                   category\n",
       "DayOfWeekMostRecentScreening               category\n",
       "HourOfDayMostRecentScreening               category\n",
       "VenueTypeMostRecentScreening               category\n",
       "FilmsTakenMostRecentScreening                 int64\n",
       "DaysFromAttendanceToResultSent                int64\n",
       "LapsedRegular3rdMostRecentEpisode          category\n",
       "LapsedRegular2ndMostRecentEpisode          category\n",
       "LapsedRegularMostRecentEpisode             category\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.7/site-packages/pandas_profiling/describe.py:392: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "  variable_stats = pd.concat(ldesc, join_axes=pd.Index([names]), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Generate pandas_profiling output for EDA\n",
    "pp = df.copy()\n",
    "\n",
    "for c in pp:\n",
    "    if pp[c].dtypes != bool and pp[c].dtypes != np.float64 and pp[c].dtypes != np.uint64 and pp[c].dtypes != np.uint64 and pp[c].dtypes != np.uint8 and pp[c].dtypes != np.datetime64 and pp[c].dtypes != np.timedelta64 and pp[c].dtypes != np.dtype('<m8[ns]'):\n",
    "        pp[c] = pp[c].astype(\"str\")\n",
    "        pp[c] = pp[c].astype(\"category\")\n",
    "\n",
    "    elif pp[c].dtypes == bool:\n",
    "        pp[c] = pp[c].astype(\"int\")\n",
    "    \n",
    "pfr = pandas_profiling.ProfileReport(pp)\n",
    "pfr.to_file(\"pandas-profiling output.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The pandas_profiling report can be viewed [here](http://htmlpreview.github.com/?https://github.com/DavidSchanzer823239622/UTS_ML2019_82329622/blob/master/pandas-profiling%20output.html \"Output from pandas_profiling\"). The report revealed no particular data problems that needed addressing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgeAtMostRecentEpisode min and max: 40 96\n",
      "AgeAtMostRecentEpisode Q1: 56.0\n",
      "AgeAtMostRecentEpisode Q2: 61.0\n",
      "AgeAtMostRecentEpisode Q3: 66.0\n",
      "AgeAtMostRecentEpisode Q4: 71.0\n",
      "AgeAtMostRecentEpisode Q5: 96.0\n",
      "TotalEpisodes min and max: 2 28\n",
      "TotalEpisodes Q1: 3.0\n",
      "TotalEpisodes Q2: 5.0\n",
      "TotalEpisodes Q3: 7.0\n",
      "TotalEpisodes Q4: 10.0\n",
      "TotalEpisodes Q5: 28.0\n",
      "DistanceKms min and max: 0.0 3406.0\n",
      "DistanceKms Q1: 2.0\n",
      "DistanceKms Q2: 3.0\n",
      "DistanceKms Q3: 6.0\n",
      "DistanceKms Q4: 11.0\n",
      "DistanceKms Q5: 3406.0\n",
      "FilmsTakenMostRecentScreening min and max: 0 19\n",
      "FilmsTakenMostRecentScreening Q1: 4.0\n",
      "FilmsTakenMostRecentScreening Q2: 4.0\n",
      "FilmsTakenMostRecentScreening Q3: 4.0\n",
      "FilmsTakenMostRecentScreening Q4: 4.0\n",
      "FilmsTakenMostRecentScreening Q5: 19.0\n",
      "DaysFromAttendanceToResultSent min and max: 0 97\n",
      "DaysFromAttendanceToResultSent Q1: 4.0\n",
      "DaysFromAttendanceToResultSent Q2: 5.0\n",
      "DaysFromAttendanceToResultSent Q3: 7.0\n",
      "DaysFromAttendanceToResultSent Q4: 9.0\n",
      "DaysFromAttendanceToResultSent Q5: 97.0\n"
     ]
    }
   ],
   "source": [
    "# In preparation for binning the numeric variables, find the required quantiles\n",
    "print(\"AgeAtMostRecentEpisode min and max:\", min(df.AgeAtMostRecentEpisode), max(df.AgeAtMostRecentEpisode))\n",
    "print(\"AgeAtMostRecentEpisode Q1:\", df.AgeAtMostRecentEpisode.quantile(.2))\n",
    "print(\"AgeAtMostRecentEpisode Q2:\", df.AgeAtMostRecentEpisode.quantile(.4))\n",
    "print(\"AgeAtMostRecentEpisode Q3:\", df.AgeAtMostRecentEpisode.quantile(.6))\n",
    "print(\"AgeAtMostRecentEpisode Q4:\", df.AgeAtMostRecentEpisode.quantile(.8))\n",
    "print(\"AgeAtMostRecentEpisode Q5:\", df.AgeAtMostRecentEpisode.quantile(1))\n",
    "\n",
    "print(\"TotalEpisodes min and max:\", min(df.TotalEpisodes), max(df.TotalEpisodes))\n",
    "print(\"TotalEpisodes Q1:\", df.TotalEpisodes.quantile(.2))\n",
    "print(\"TotalEpisodes Q2:\", df.TotalEpisodes.quantile(.4))\n",
    "print(\"TotalEpisodes Q3:\", df.TotalEpisodes.quantile(.6))\n",
    "print(\"TotalEpisodes Q4:\", df.TotalEpisodes.quantile(.8))\n",
    "print(\"TotalEpisodes Q5:\", df.TotalEpisodes.quantile(1))\n",
    "\n",
    "print(\"DistanceKms min and max:\", min(df.DistanceKms), max(df.DistanceKms))\n",
    "print(\"DistanceKms Q1:\", df.DistanceKms.quantile(.2))\n",
    "print(\"DistanceKms Q2:\", df.DistanceKms.quantile(.4))\n",
    "print(\"DistanceKms Q3:\", df.DistanceKms.quantile(.6))\n",
    "print(\"DistanceKms Q4:\", df.DistanceKms.quantile(.8))\n",
    "print(\"DistanceKms Q5:\", df.DistanceKms.quantile(1))\n",
    "\n",
    "print(\"FilmsTakenMostRecentScreening min and max:\", min(df.FilmsTakenMostRecentScreening), max(df.FilmsTakenMostRecentScreening))\n",
    "print(\"FilmsTakenMostRecentScreening Q1:\", df.FilmsTakenMostRecentScreening.quantile(.2))\n",
    "print(\"FilmsTakenMostRecentScreening Q2:\", df.FilmsTakenMostRecentScreening.quantile(.4))\n",
    "print(\"FilmsTakenMostRecentScreening Q3:\", df.FilmsTakenMostRecentScreening.quantile(.6))\n",
    "print(\"FilmsTakenMostRecentScreening Q4:\", df.FilmsTakenMostRecentScreening.quantile(.8))\n",
    "print(\"FilmsTakenMostRecentScreening Q5:\", df.FilmsTakenMostRecentScreening.quantile(1))\n",
    "\n",
    "print(\"DaysFromAttendanceToResultSent min and max:\", min(df.DaysFromAttendanceToResultSent), max(df.DaysFromAttendanceToResultSent))\n",
    "print(\"DaysFromAttendanceToResultSent Q1:\", df.DaysFromAttendanceToResultSent.quantile(.2))\n",
    "print(\"DaysFromAttendanceToResultSent Q2:\", df.DaysFromAttendanceToResultSent.quantile(.4))\n",
    "print(\"DaysFromAttendanceToResultSent Q3:\", df.DaysFromAttendanceToResultSent.quantile(.6))\n",
    "print(\"DaysFromAttendanceToResultSent Q4:\", df.DaysFromAttendanceToResultSent.quantile(.8))\n",
    "print(\"DaysFromAttendanceToResultSent Q5:\", df.DaysFromAttendanceToResultSent.quantile(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountryOfBirth                             category\n",
      "MainLanguage                               category\n",
      "IndigenousStatus                           category\n",
      "RemotenessArea                             category\n",
      "RelativeSocioEconomicDisadvantageDecile    category\n",
      "HistoryPreviousCancer                          bool\n",
      "HistoryFamilyCancer                            bool\n",
      "HasBeenDNA                                     bool\n",
      "HadAssessment                                  bool\n",
      "HadNeedleBiopsy                                bool\n",
      "HadTechRecall                                  bool\n",
      "MonthMostRecentScreening                   category\n",
      "DayOfWeekMostRecentScreening               category\n",
      "HourOfDayMostRecentScreening               category\n",
      "VenueTypeMostRecentScreening               category\n",
      "LapsedRegular3rdMostRecentEpisode          category\n",
      "LapsedRegular2ndMostRecentEpisode          category\n",
      "LapsedRegularMostRecentEpisode             category\n",
      "AgeAtMostRecentEpisode_lt56                   int64\n",
      "AgeAtMostRecentEpisode_56_to_60               int64\n",
      "AgeAtMostRecentEpisode_61_to_65               int64\n",
      "AgeAtMostRecentEpisode_66_to_70               int64\n",
      "AgeAtMostRecentEpisode_ge71                   int64\n",
      "TotalEpisodes_lt3                             int64\n",
      "TotalEpisodes_3_to_4                          int64\n",
      "TotalEpisodes_5_to_6                          int64\n",
      "TotalEpisodes_7_to_9                          int64\n",
      "TotalEpisodes_ge10                            int64\n",
      "DistanceKms_lt1                             float64\n",
      "DistanceKms_1_to_lt3                        float64\n",
      "DistanceKms_3_to_lt6                        float64\n",
      "DistanceKms_6_to_lt11                       float64\n",
      "DistanceKms_ge11                            float64\n",
      "FilmsTakenMostRecentScreening_lt4             int64\n",
      "FilmsTakenMostRecentScreening_4               int64\n",
      "FilmsTakenMostRecentScreening_5               int64\n",
      "FilmsTakenMostRecentScreening_6_to_7          int64\n",
      "FilmsTakenMostRecentScreening_ge8             int64\n",
      "DaysFromAttendanceToResultSent_lt4            int64\n",
      "DaysFromAttendanceToResultSent_4              int64\n",
      "DaysFromAttendanceToResultSent_5_to_6         int64\n",
      "DaysFromAttendanceToResultSent_7_to_8         int64\n",
      "DaysFromAttendanceToResultSent_ge9            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Bin the numeric variables, to aid interpretability of the Feature Importance chart.\n",
    "\n",
    "# Firstly, put the AgeAtMostRecentEpisode values into 5 equal depth buckets\n",
    "df['AgeAtMostRecentEpisode_lt56'] = df.AgeAtMostRecentEpisode[df.AgeAtMostRecentEpisode < df.AgeAtMostRecentEpisode.quantile(.2)]\n",
    "df['AgeAtMostRecentEpisode_56_to_60'] = df.AgeAtMostRecentEpisode[(df.AgeAtMostRecentEpisode >= df.AgeAtMostRecentEpisode.quantile(.2)) & (df.AgeAtMostRecentEpisode < df.AgeAtMostRecentEpisode.quantile(.4))]\n",
    "df['AgeAtMostRecentEpisode_61_to_65'] = df.AgeAtMostRecentEpisode[(df.AgeAtMostRecentEpisode >= df.AgeAtMostRecentEpisode.quantile(.4)) & (df.AgeAtMostRecentEpisode < df.AgeAtMostRecentEpisode.quantile(.6))]\n",
    "df['AgeAtMostRecentEpisode_66_to_70'] = df.AgeAtMostRecentEpisode[(df.AgeAtMostRecentEpisode >= df.AgeAtMostRecentEpisode.quantile(.6)) & (df.AgeAtMostRecentEpisode < df.AgeAtMostRecentEpisode.quantile(.8))]\n",
    "df['AgeAtMostRecentEpisode_ge71'] = df.AgeAtMostRecentEpisode[df.AgeAtMostRecentEpisode >= df.AgeAtMostRecentEpisode.quantile(.8)]\n",
    "df['AgeAtMostRecentEpisode_lt56'] = df['AgeAtMostRecentEpisode_lt56'].values.astype(np.int64)\n",
    "df['AgeAtMostRecentEpisode_56_to_60'] = df['AgeAtMostRecentEpisode_56_to_60'].values.astype(np.int64)\n",
    "df['AgeAtMostRecentEpisode_61_to_65'] = df['AgeAtMostRecentEpisode_61_to_65'].values.astype(np.int64)\n",
    "df['AgeAtMostRecentEpisode_66_to_70'] = df['AgeAtMostRecentEpisode_66_to_70'].values.astype(np.int64)\n",
    "df['AgeAtMostRecentEpisode_ge71'] = df['AgeAtMostRecentEpisode_ge71'].values.astype(np.int64)\n",
    "df = df.drop(['AgeAtMostRecentEpisode'], axis = 1)\n",
    "\n",
    "# Now, put the TotalEpisodes values into 5 equal depth buckets\n",
    "df['TotalEpisodes_lt3'] = df.TotalEpisodes[df.TotalEpisodes < df.TotalEpisodes.quantile(.2)]\n",
    "df['TotalEpisodes_3_to_4'] = df.TotalEpisodes[(df.TotalEpisodes >= df.TotalEpisodes.quantile(.2)) & (df.TotalEpisodes < df.TotalEpisodes.quantile(.4))]\n",
    "df['TotalEpisodes_5_to_6'] = df.TotalEpisodes[(df.TotalEpisodes >= df.TotalEpisodes.quantile(.4)) & (df.TotalEpisodes < df.TotalEpisodes.quantile(.6))]\n",
    "df['TotalEpisodes_7_to_9'] = df.TotalEpisodes[(df.TotalEpisodes >= df.TotalEpisodes.quantile(.6)) & (df.TotalEpisodes < df.TotalEpisodes.quantile(.8))]\n",
    "df['TotalEpisodes_ge10'] = df.TotalEpisodes[df.TotalEpisodes >= df.TotalEpisodes.quantile(.8)]\n",
    "df['TotalEpisodes_lt3'] = df['TotalEpisodes_lt3'].values.astype(np.int64)\n",
    "df['TotalEpisodes_3_to_4'] = df['TotalEpisodes_3_to_4'].values.astype(np.int64)\n",
    "df['TotalEpisodes_5_to_6'] = df['TotalEpisodes_5_to_6'].values.astype(np.int64)\n",
    "df['TotalEpisodes_7_to_9'] = df['TotalEpisodes_7_to_9'].values.astype(np.int64)\n",
    "df['TotalEpisodes_ge10'] = df['TotalEpisodes_ge10'].values.astype(np.int64)\n",
    "df = df.drop(['TotalEpisodes'], axis = 1)\n",
    "\n",
    "# Next, put the DistanceKms values into 5 equal depth buckets\n",
    "df['DistanceKms_lt1'] = df.DistanceKms[df.DistanceKms < df.DistanceKms.quantile(.2)]\n",
    "df['DistanceKms_1_to_lt3'] = df.DistanceKms[(df.DistanceKms >= df.DistanceKms.quantile(.2)) & (df.DistanceKms < df.DistanceKms.quantile(.4))]\n",
    "df['DistanceKms_3_to_lt6'] = df.DistanceKms[(df.DistanceKms >= df.DistanceKms.quantile(.4)) & (df.DistanceKms < df.DistanceKms.quantile(.6))]\n",
    "df['DistanceKms_6_to_lt11'] = df.DistanceKms[(df.DistanceKms >= df.DistanceKms.quantile(.6)) & (df.DistanceKms < df.DistanceKms.quantile(.8))]\n",
    "df['DistanceKms_ge11'] = df.DistanceKms[df.DistanceKms >= df.DistanceKms.quantile(.8)]\n",
    "df['DistanceKms_lt1'] = df['DistanceKms_lt1'].values.astype(np.float64)\n",
    "df['DistanceKms_1_to_lt3'] = df['DistanceKms_1_to_lt3'].values.astype(np.float64)\n",
    "df['DistanceKms_3_to_lt6'] = df['DistanceKms_3_to_lt6'].values.astype(np.float64)\n",
    "df['DistanceKms_6_to_lt11'] = df['DistanceKms_6_to_lt11'].values.astype(np.float64)\n",
    "df['DistanceKms_ge11'] = df['DistanceKms_ge11'].values.astype(np.float64)\n",
    "df = df.drop(['DistanceKms'], axis = 1)\n",
    "\n",
    "# Next, put the FilmsTakenMostRecentScreening values into 5 non-equal depth buckets (due to extreme skew)\n",
    "df['FilmsTakenMostRecentScreening_lt4'] = df.FilmsTakenMostRecentScreening[df.FilmsTakenMostRecentScreening < 4]\n",
    "df['FilmsTakenMostRecentScreening_4'] = df.FilmsTakenMostRecentScreening[(df.FilmsTakenMostRecentScreening == 4)]\n",
    "df['FilmsTakenMostRecentScreening_5'] = df.FilmsTakenMostRecentScreening[(df.FilmsTakenMostRecentScreening == 5)]\n",
    "df['FilmsTakenMostRecentScreening_6_to_7'] = df.FilmsTakenMostRecentScreening[(df.FilmsTakenMostRecentScreening >= 6) & (df.FilmsTakenMostRecentScreening <= 7)]\n",
    "df['FilmsTakenMostRecentScreening_ge8'] = df.FilmsTakenMostRecentScreening[df.FilmsTakenMostRecentScreening >= 8]\n",
    "df['FilmsTakenMostRecentScreening_lt4'] = df['FilmsTakenMostRecentScreening_lt4'].values.astype(np.int64)\n",
    "df['FilmsTakenMostRecentScreening_4'] = df['FilmsTakenMostRecentScreening_4'].values.astype(np.int64)\n",
    "df['FilmsTakenMostRecentScreening_5'] = df['FilmsTakenMostRecentScreening_5'].values.astype(np.int64)\n",
    "df['FilmsTakenMostRecentScreening_6_to_7'] = df['FilmsTakenMostRecentScreening_6_to_7'].values.astype(np.int64)\n",
    "df['FilmsTakenMostRecentScreening_ge8'] = df['FilmsTakenMostRecentScreening_ge8'].values.astype(np.int64)\n",
    "df = df.drop(['FilmsTakenMostRecentScreening'], axis = 1)\n",
    "    \n",
    "# Finally, put the DaysFromAttendanceToResultSent values into 5 equal depth buckets\n",
    "df['DaysFromAttendanceToResultSent_lt4'] = df.DaysFromAttendanceToResultSent[df.DaysFromAttendanceToResultSent < df.DaysFromAttendanceToResultSent.quantile(.2)]\n",
    "df['DaysFromAttendanceToResultSent_4'] = df.DaysFromAttendanceToResultSent[(df.DaysFromAttendanceToResultSent >= df.DaysFromAttendanceToResultSent.quantile(.2)) & (df.DaysFromAttendanceToResultSent < df.DaysFromAttendanceToResultSent.quantile(.4))]\n",
    "df['DaysFromAttendanceToResultSent_5_to_6'] = df.DaysFromAttendanceToResultSent[(df.DaysFromAttendanceToResultSent >= df.DaysFromAttendanceToResultSent.quantile(.4)) & (df.DaysFromAttendanceToResultSent < df.DaysFromAttendanceToResultSent.quantile(.6))]\n",
    "df['DaysFromAttendanceToResultSent_7_to_8'] = df.DaysFromAttendanceToResultSent[(df.DaysFromAttendanceToResultSent >= df.DaysFromAttendanceToResultSent.quantile(.6)) & (df.DaysFromAttendanceToResultSent < df.DaysFromAttendanceToResultSent.quantile(.8))]\n",
    "df['DaysFromAttendanceToResultSent_ge9'] = df.DaysFromAttendanceToResultSent[df.DaysFromAttendanceToResultSent >= df.DaysFromAttendanceToResultSent.quantile(.8)]\n",
    "df['DaysFromAttendanceToResultSent_lt4'] = df['DaysFromAttendanceToResultSent_lt4'].values.astype(np.int64)\n",
    "df['DaysFromAttendanceToResultSent_4'] = df['DaysFromAttendanceToResultSent_4'].values.astype(np.int64)\n",
    "df['DaysFromAttendanceToResultSent_5_to_6'] = df['DaysFromAttendanceToResultSent_5_to_6'].values.astype(np.int64)\n",
    "df['DaysFromAttendanceToResultSent_7_to_8'] = df['DaysFromAttendanceToResultSent_7_to_8'].values.astype(np.int64)\n",
    "df['DaysFromAttendanceToResultSent_ge9'] = df['DaysFromAttendanceToResultSent_ge9'].values.astype(np.int64)\n",
    "df = df.drop(['DaysFromAttendanceToResultSent'], axis = 1)\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HistoryPreviousCancer',\n",
       " 'HistoryFamilyCancer',\n",
       " 'HasBeenDNA',\n",
       " 'HadAssessment',\n",
       " 'HadNeedleBiopsy',\n",
       " 'HadTechRecall',\n",
       " 'LapsedRegularMostRecentEpisode',\n",
       " 'AgeAtMostRecentEpisode_lt56',\n",
       " 'AgeAtMostRecentEpisode_56_to_60',\n",
       " 'AgeAtMostRecentEpisode_61_to_65',\n",
       " 'AgeAtMostRecentEpisode_66_to_70',\n",
       " 'AgeAtMostRecentEpisode_ge71',\n",
       " 'TotalEpisodes_lt3',\n",
       " 'TotalEpisodes_3_to_4',\n",
       " 'TotalEpisodes_5_to_6',\n",
       " 'TotalEpisodes_7_to_9',\n",
       " 'TotalEpisodes_ge10',\n",
       " 'DistanceKms_lt1',\n",
       " 'DistanceKms_1_to_lt3',\n",
       " 'DistanceKms_3_to_lt6',\n",
       " 'DistanceKms_6_to_lt11',\n",
       " 'DistanceKms_ge11',\n",
       " 'FilmsTakenMostRecentScreening_lt4',\n",
       " 'FilmsTakenMostRecentScreening_4',\n",
       " 'FilmsTakenMostRecentScreening_5',\n",
       " 'FilmsTakenMostRecentScreening_6_to_7',\n",
       " 'FilmsTakenMostRecentScreening_ge8',\n",
       " 'DaysFromAttendanceToResultSent_lt4',\n",
       " 'DaysFromAttendanceToResultSent_4',\n",
       " 'DaysFromAttendanceToResultSent_5_to_6',\n",
       " 'DaysFromAttendanceToResultSent_7_to_8',\n",
       " 'DaysFromAttendanceToResultSent_ge9',\n",
       " 'CountryOfBirth_AFGHANISTAN',\n",
       " 'CountryOfBirth_ALBANIA',\n",
       " 'CountryOfBirth_ALGERIA',\n",
       " 'CountryOfBirth_ARGENTINA',\n",
       " 'CountryOfBirth_ARMENIA',\n",
       " 'CountryOfBirth_ARUBA',\n",
       " 'CountryOfBirth_AUSTRALIA',\n",
       " 'CountryOfBirth_AUSTRIA',\n",
       " 'CountryOfBirth_BANGLADESH',\n",
       " 'CountryOfBirth_BELGIUM',\n",
       " 'CountryOfBirth_BHUTAN',\n",
       " 'CountryOfBirth_BOLIVIA, PLURINATIONAL STATE OF',\n",
       " 'CountryOfBirth_BOSNIA AND HERZEGOVINA',\n",
       " 'CountryOfBirth_BRAZIL',\n",
       " 'CountryOfBirth_BRUNEI DARUSSALAM',\n",
       " 'CountryOfBirth_BULGARIA',\n",
       " 'CountryOfBirth_CAMBODIA',\n",
       " 'CountryOfBirth_CANADA',\n",
       " 'CountryOfBirth_CENTRAL AFRICAN REPUBLIC',\n",
       " 'CountryOfBirth_CHILE',\n",
       " 'CountryOfBirth_CHINA',\n",
       " 'CountryOfBirth_COCOS (KEELING) ISLANDS',\n",
       " 'CountryOfBirth_COLOMBIA',\n",
       " 'CountryOfBirth_CONGO',\n",
       " 'CountryOfBirth_CONGO, THE DEMOCRATIC REPUBLIC OF THE',\n",
       " 'CountryOfBirth_COOK ISLANDS',\n",
       " 'CountryOfBirth_COSTA RICA',\n",
       " 'CountryOfBirth_CROATIA',\n",
       " 'CountryOfBirth_CYPRUS',\n",
       " 'CountryOfBirth_CZECH REPUBLIC',\n",
       " 'CountryOfBirth_DENMARK',\n",
       " 'CountryOfBirth_ECUADOR',\n",
       " 'CountryOfBirth_EGYPT',\n",
       " 'CountryOfBirth_EL SALVADOR',\n",
       " 'CountryOfBirth_ESTONIA',\n",
       " 'CountryOfBirth_ETHIOPIA',\n",
       " 'CountryOfBirth_FIJI',\n",
       " 'CountryOfBirth_FINLAND',\n",
       " 'CountryOfBirth_FRANCE',\n",
       " 'CountryOfBirth_GEORGIA',\n",
       " 'CountryOfBirth_GERMANY',\n",
       " 'CountryOfBirth_GHANA',\n",
       " 'CountryOfBirth_GIBRALTAR',\n",
       " 'CountryOfBirth_GREECE',\n",
       " 'CountryOfBirth_GRENADA',\n",
       " 'CountryOfBirth_GUINEA',\n",
       " 'CountryOfBirth_HONDURAS',\n",
       " 'CountryOfBirth_HONG KONG',\n",
       " 'CountryOfBirth_HUNGARY',\n",
       " 'CountryOfBirth_INDIA',\n",
       " 'CountryOfBirth_INDONESIA',\n",
       " 'CountryOfBirth_IRAN',\n",
       " 'CountryOfBirth_IRAQ',\n",
       " 'CountryOfBirth_IRELAND',\n",
       " 'CountryOfBirth_ISLE OF MAN',\n",
       " 'CountryOfBirth_ISRAEL',\n",
       " 'CountryOfBirth_ITALY',\n",
       " 'CountryOfBirth_JAMAICA',\n",
       " 'CountryOfBirth_JAPAN',\n",
       " 'CountryOfBirth_JORDAN',\n",
       " 'CountryOfBirth_KENYA',\n",
       " 'CountryOfBirth_KIRIBATI',\n",
       " \"CountryOfBirth_KOREA, DEMOCRATIC PEOPLE'S REPUBLIC OF\",\n",
       " 'CountryOfBirth_KOREA, REPUBLIC OF',\n",
       " 'CountryOfBirth_KUWAIT',\n",
       " 'CountryOfBirth_KYRGYZSTAN',\n",
       " \"CountryOfBirth_LAO PEOPLE'S DEMOCRATIC REPUBLIC\",\n",
       " 'CountryOfBirth_LATVIA',\n",
       " 'CountryOfBirth_LEBANON',\n",
       " 'CountryOfBirth_LESOTHO',\n",
       " 'CountryOfBirth_LIBERIA',\n",
       " 'CountryOfBirth_LIBYAN ARAB JAMAHIRIYA',\n",
       " 'CountryOfBirth_LITHUANIA',\n",
       " 'CountryOfBirth_MACAO',\n",
       " 'CountryOfBirth_MACEDONIA, THE FORMER YUGOSLAV REPUBLIC OF',\n",
       " 'CountryOfBirth_MALAWI',\n",
       " 'CountryOfBirth_MALAYSIA',\n",
       " 'CountryOfBirth_MALTA',\n",
       " 'CountryOfBirth_MAURITIUS',\n",
       " 'CountryOfBirth_MEXICO',\n",
       " 'CountryOfBirth_MONTENEGRO',\n",
       " 'CountryOfBirth_MOROCCO',\n",
       " 'CountryOfBirth_MOZAMBIQUE',\n",
       " 'CountryOfBirth_MYANMAR',\n",
       " 'CountryOfBirth_NAMIBIA',\n",
       " 'CountryOfBirth_NEPAL',\n",
       " 'CountryOfBirth_NETHERLANDS',\n",
       " 'CountryOfBirth_NEW CALEDONIA',\n",
       " 'CountryOfBirth_NEW ZEALAND',\n",
       " 'CountryOfBirth_NICARAGUA',\n",
       " 'CountryOfBirth_NIGERIA',\n",
       " 'CountryOfBirth_NORFOLK ISLAND',\n",
       " 'CountryOfBirth_NORWAY',\n",
       " 'CountryOfBirth_PAKISTAN',\n",
       " 'CountryOfBirth_PALESTINIAN TERRITORY, OCCUPIED',\n",
       " 'CountryOfBirth_PANAMA',\n",
       " 'CountryOfBirth_PAPUA NEW GUINEA',\n",
       " 'CountryOfBirth_PARAGUAY',\n",
       " 'CountryOfBirth_PERU',\n",
       " 'CountryOfBirth_PHILIPPINES',\n",
       " 'CountryOfBirth_POLAND',\n",
       " 'CountryOfBirth_PORTUGAL',\n",
       " 'CountryOfBirth_ROMANIA',\n",
       " 'CountryOfBirth_RUSSIAN FEDERATION',\n",
       " 'CountryOfBirth_SAMOA',\n",
       " 'CountryOfBirth_SAUDI ARABIA',\n",
       " 'CountryOfBirth_SENEGAL',\n",
       " 'CountryOfBirth_SERBIA',\n",
       " 'CountryOfBirth_SEYCHELLES',\n",
       " 'CountryOfBirth_SINGAPORE',\n",
       " 'CountryOfBirth_SLOVAKIA',\n",
       " 'CountryOfBirth_SLOVENIA',\n",
       " 'CountryOfBirth_SOLOMON ISLANDS',\n",
       " 'CountryOfBirth_SOUTH AFRICA',\n",
       " 'CountryOfBirth_SPAIN',\n",
       " 'CountryOfBirth_SRI LANKA',\n",
       " 'CountryOfBirth_SUDAN',\n",
       " 'CountryOfBirth_SWEDEN',\n",
       " 'CountryOfBirth_SWITZERLAND',\n",
       " 'CountryOfBirth_SYRIAN ARAB REPUBLIC',\n",
       " 'CountryOfBirth_TAIWAN, PROVINCE OF CHINA',\n",
       " 'CountryOfBirth_TANZANIA, UNITED REPUBLIC OF',\n",
       " 'CountryOfBirth_THAILAND',\n",
       " 'CountryOfBirth_TIMOR-LESTE',\n",
       " 'CountryOfBirth_TONGA',\n",
       " 'CountryOfBirth_TRINIDAD AND TOBAGO',\n",
       " 'CountryOfBirth_TUNISIA',\n",
       " 'CountryOfBirth_TURKEY',\n",
       " 'CountryOfBirth_UGANDA',\n",
       " 'CountryOfBirth_UKRAINE',\n",
       " 'CountryOfBirth_UNITED KINGDOM',\n",
       " 'CountryOfBirth_UNITED STATES',\n",
       " 'CountryOfBirth_UNITED STATES MINOR OUTLYING ISLANDS',\n",
       " 'CountryOfBirth_URUGUAY',\n",
       " 'CountryOfBirth_Unknown',\n",
       " 'CountryOfBirth_VANUATU',\n",
       " 'CountryOfBirth_VENEZUELA, BOLIVARIAN REPUBLIC OF',\n",
       " 'CountryOfBirth_VIET NAM',\n",
       " 'CountryOfBirth_YEMEN',\n",
       " 'CountryOfBirth_ZIMBABWE',\n",
       " 'CountryOfBirth_ANGOLA',\n",
       " 'CountryOfBirth_BAHAMAS',\n",
       " 'CountryOfBirth_BAHRAIN',\n",
       " 'CountryOfBirth_BELARUS',\n",
       " 'CountryOfBirth_CHRISTMAS ISLAND',\n",
       " 'CountryOfBirth_CUBA',\n",
       " 'CountryOfBirth_DOMINICAN REPUBLIC',\n",
       " 'CountryOfBirth_FRENCH GUIANA',\n",
       " 'CountryOfBirth_GAMBIA',\n",
       " 'CountryOfBirth_GUYANA',\n",
       " 'CountryOfBirth_ICELAND',\n",
       " 'CountryOfBirth_JERSEY',\n",
       " 'CountryOfBirth_NIUE',\n",
       " 'CountryOfBirth_SIERRA LEONE',\n",
       " 'CountryOfBirth_SOMALIA',\n",
       " 'CountryOfBirth_SWAZILAND',\n",
       " 'CountryOfBirth_UZBEKISTAN',\n",
       " 'CountryOfBirth_WALLIS AND FUTUNA',\n",
       " 'CountryOfBirth_ZAMBIA',\n",
       " 'CountryOfBirth_AZERBAIJAN',\n",
       " 'CountryOfBirth_BARBADOS',\n",
       " 'CountryOfBirth_ERITREA',\n",
       " 'CountryOfBirth_FRENCH POLYNESIA',\n",
       " 'CountryOfBirth_GUERNSEY',\n",
       " 'CountryOfBirth_MOLDOVA, REPUBLIC OF',\n",
       " 'CountryOfBirth_NETHERLANDS ANTILLES',\n",
       " 'CountryOfBirth_QATAR',\n",
       " 'CountryOfBirth_BERMUDA',\n",
       " 'CountryOfBirth_GUATEMALA',\n",
       " 'CountryOfBirth_HAITI',\n",
       " 'CountryOfBirth_MONACO',\n",
       " 'CountryOfBirth_NAURU',\n",
       " 'CountryOfBirth_REUNION',\n",
       " 'CountryOfBirth_SURINAME',\n",
       " 'CountryOfBirth_UNITED ARAB EMIRATES',\n",
       " 'CountryOfBirth_DJIBOUTI',\n",
       " 'CountryOfBirth_GUAM',\n",
       " 'CountryOfBirth_KAZAKHSTAN',\n",
       " 'CountryOfBirth_LUXEMBOURG',\n",
       " 'CountryOfBirth_MONGOLIA',\n",
       " 'CountryOfBirth_SAINT HELENA',\n",
       " 'CountryOfBirth_SAO TOME AND PRINCIPE',\n",
       " 'CountryOfBirth_TOKELAU',\n",
       " 'CountryOfBirth_VIRGIN ISLANDS, U.S.',\n",
       " 'CountryOfBirth_BURUNDI',\n",
       " 'CountryOfBirth_MALDIVES',\n",
       " 'CountryOfBirth_MADAGASCAR',\n",
       " 'CountryOfBirth_CAMEROON',\n",
       " 'CountryOfBirth_PUERTO RICO',\n",
       " 'CountryOfBirth_SAINT KITTS AND NEVIS',\n",
       " 'CountryOfBirth_TOGO',\n",
       " 'CountryOfBirth_ALAND ISLANDS',\n",
       " 'CountryOfBirth_ANDORRA',\n",
       " 'CountryOfBirth_BOTSWANA',\n",
       " 'CountryOfBirth_TUVALU',\n",
       " 'CountryOfBirth_SOUTH GEORGIA AND THE SOUTH SANDWICH ISLANDS',\n",
       " 'CountryOfBirth_BELIZE',\n",
       " 'CountryOfBirth_FALKLAND ISLANDS (MALVINAS)',\n",
       " 'CountryOfBirth_MAURITANIA',\n",
       " 'CountryOfBirth_OMAN',\n",
       " 'CountryOfBirth_AMERICAN SAMOA',\n",
       " \"CountryOfBirth_COTE D'IVOIRE\",\n",
       " 'CountryOfBirth_GREENLAND',\n",
       " 'CountryOfBirth_BENIN',\n",
       " 'CountryOfBirth_GUINEA-BISSAU',\n",
       " 'CountryOfBirth_TAJIKISTAN',\n",
       " 'CountryOfBirth_VIRGIN ISLANDS, BRITISH',\n",
       " 'CountryOfBirth_RWANDA',\n",
       " 'CountryOfBirth_BRITISH INDIAN OCEAN TERRITORY',\n",
       " 'CountryOfBirth_FAROE ISLANDS',\n",
       " 'CountryOfBirth_MALI',\n",
       " 'CountryOfBirth_MICRONESIA, FEDERATED STATES OF',\n",
       " 'CountryOfBirth_COMOROS',\n",
       " 'CountryOfBirth_CHAD',\n",
       " 'CountryOfBirth_GUADELOUPE',\n",
       " 'MainLanguage_Arabic',\n",
       " 'MainLanguage_Cantonese',\n",
       " 'MainLanguage_Croatian',\n",
       " 'MainLanguage_English Only',\n",
       " 'MainLanguage_French',\n",
       " 'MainLanguage_German',\n",
       " 'MainLanguage_Greek',\n",
       " 'MainLanguage_Hindi',\n",
       " 'MainLanguage_Indonesian',\n",
       " 'MainLanguage_Italian',\n",
       " 'MainLanguage_Korean',\n",
       " 'MainLanguage_Macedonian',\n",
       " 'MainLanguage_Maltese',\n",
       " 'MainLanguage_Mandarin',\n",
       " 'MainLanguage_Other (please specify)',\n",
       " 'MainLanguage_Serbian',\n",
       " 'MainLanguage_Spanish',\n",
       " 'MainLanguage_Tagalog (Filipino)',\n",
       " 'MainLanguage_Tamil',\n",
       " 'MainLanguage_Turkish',\n",
       " 'MainLanguage_Unknown',\n",
       " 'MainLanguage_Vietnamese',\n",
       " 'IndigenousStatus_Aboriginal',\n",
       " 'IndigenousStatus_Aboriginal and Torres Strait Islander',\n",
       " 'IndigenousStatus_Declines to Respond',\n",
       " 'IndigenousStatus_Non-indigenous',\n",
       " 'IndigenousStatus_Not stated',\n",
       " 'IndigenousStatus_Torres Strait Islander',\n",
       " 'RemotenessArea_Inner Regional Australia',\n",
       " 'RemotenessArea_Major Cities of Australia',\n",
       " 'RemotenessArea_Outer Regional Australia',\n",
       " 'RemotenessArea_Remote Australia',\n",
       " 'RemotenessArea_Very Remote Australia',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_0',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_1',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_10',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_2',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_3',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_4',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_5',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_6',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_7',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_8',\n",
       " 'RelativeSocioEconomicDisadvantageDecile_9',\n",
       " 'MonthMostRecentScreening_0',\n",
       " 'MonthMostRecentScreening_1',\n",
       " 'MonthMostRecentScreening_10',\n",
       " 'MonthMostRecentScreening_11',\n",
       " 'MonthMostRecentScreening_12',\n",
       " 'MonthMostRecentScreening_2',\n",
       " 'MonthMostRecentScreening_3',\n",
       " 'MonthMostRecentScreening_4',\n",
       " 'MonthMostRecentScreening_5',\n",
       " 'MonthMostRecentScreening_6',\n",
       " 'MonthMostRecentScreening_7',\n",
       " 'MonthMostRecentScreening_8',\n",
       " 'MonthMostRecentScreening_9',\n",
       " 'DayOfWeekMostRecentScreening_0',\n",
       " 'DayOfWeekMostRecentScreening_1',\n",
       " 'DayOfWeekMostRecentScreening_2',\n",
       " 'DayOfWeekMostRecentScreening_3',\n",
       " 'DayOfWeekMostRecentScreening_4',\n",
       " 'DayOfWeekMostRecentScreening_5',\n",
       " 'DayOfWeekMostRecentScreening_6',\n",
       " 'DayOfWeekMostRecentScreening_7',\n",
       " 'HourOfDayMostRecentScreening_0',\n",
       " 'HourOfDayMostRecentScreening_10',\n",
       " 'HourOfDayMostRecentScreening_11',\n",
       " 'HourOfDayMostRecentScreening_12',\n",
       " 'HourOfDayMostRecentScreening_13',\n",
       " 'HourOfDayMostRecentScreening_14',\n",
       " 'HourOfDayMostRecentScreening_15',\n",
       " 'HourOfDayMostRecentScreening_16',\n",
       " 'HourOfDayMostRecentScreening_17',\n",
       " 'HourOfDayMostRecentScreening_18',\n",
       " 'HourOfDayMostRecentScreening_19',\n",
       " 'HourOfDayMostRecentScreening_7',\n",
       " 'HourOfDayMostRecentScreening_8',\n",
       " 'HourOfDayMostRecentScreening_9',\n",
       " 'HourOfDayMostRecentScreening_6',\n",
       " 'HourOfDayMostRecentScreening_20',\n",
       " 'HourOfDayMostRecentScreening_2',\n",
       " 'HourOfDayMostRecentScreening_22',\n",
       " 'HourOfDayMostRecentScreening_3',\n",
       " 'HourOfDayMostRecentScreening_23',\n",
       " 'HourOfDayMostRecentScreening_1',\n",
       " 'HourOfDayMostRecentScreening_21',\n",
       " 'VenueTypeMostRecentScreening_Fixed',\n",
       " 'VenueTypeMostRecentScreening_Mobile',\n",
       " 'LapsedRegular3rdMostRecentEpisode_Lapsed',\n",
       " 'LapsedRegular3rdMostRecentEpisode_Regular',\n",
       " 'LapsedRegular2ndMostRecentEpisode_Lapsed',\n",
       " 'LapsedRegular2ndMostRecentEpisode_Regular']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to clean missing data or remove correlated features,\n",
    "# as boosted trees (which is what we will be using) are robust to these potential data problems\n",
    "\n",
    "# Perform one-hot encoding of all categorical columns\n",
    "\n",
    "# CountryOfBirth                             \n",
    "one_hot = pd.get_dummies(df['CountryOfBirth'], prefix = 'CountryOfBirth')\n",
    "df = df.drop('CountryOfBirth',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# MainLanguage                             \n",
    "one_hot = pd.get_dummies(df['MainLanguage'], prefix = 'MainLanguage')\n",
    "df = df.drop('MainLanguage',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# IndigenousStatus                             \n",
    "one_hot = pd.get_dummies(df['IndigenousStatus'], prefix = 'IndigenousStatus')\n",
    "df = df.drop('IndigenousStatus',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# RemotenessArea                             \n",
    "one_hot = pd.get_dummies(df['RemotenessArea'], prefix = 'RemotenessArea')\n",
    "df = df.drop('RemotenessArea',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# RelativeSocioEconomicDisadvantageDecile                             \n",
    "one_hot = pd.get_dummies(df['RelativeSocioEconomicDisadvantageDecile'], prefix = 'RelativeSocioEconomicDisadvantageDecile')\n",
    "df = df.drop('RelativeSocioEconomicDisadvantageDecile',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# MonthMostRecentScreening                             \n",
    "one_hot = pd.get_dummies(df['MonthMostRecentScreening'], prefix = 'MonthMostRecentScreening')\n",
    "df = df.drop('MonthMostRecentScreening',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# DayOfWeekMostRecentScreening                             \n",
    "one_hot = pd.get_dummies(df['DayOfWeekMostRecentScreening'], prefix = 'DayOfWeekMostRecentScreening')\n",
    "df = df.drop('DayOfWeekMostRecentScreening',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# HourOfDayMostRecentScreening                             \n",
    "one_hot = pd.get_dummies(df['HourOfDayMostRecentScreening'], prefix = 'HourOfDayMostRecentScreening')\n",
    "df = df.drop('HourOfDayMostRecentScreening',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# VenueTypeMostRecentScreening                             \n",
    "one_hot = pd.get_dummies(df['VenueTypeMostRecentScreening'], prefix = 'VenueTypeMostRecentScreening')\n",
    "df = df.drop('VenueTypeMostRecentScreening',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# LapsedRegular3rdMostRecentEpisode                             \n",
    "one_hot = pd.get_dummies(df['LapsedRegular3rdMostRecentEpisode'], prefix = 'LapsedRegular3rdMostRecentEpisode')\n",
    "df = df.drop('LapsedRegular3rdMostRecentEpisode',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "# LapsedRegular2ndMostRecentEpisode                             \n",
    "one_hot = pd.get_dummies(df['LapsedRegular2ndMostRecentEpisode'], prefix = 'LapsedRegular2ndMostRecentEpisode')\n",
    "df = df.drop('LapsedRegular2ndMostRecentEpisode',axis = 1)\n",
    "df = df.join(one_hot)\n",
    "\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 340)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 339)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into X and y\n",
    "TargetVariable = 'LapsedRegularMostRecentEpisode'\n",
    "X = df.loc[:, df.columns != TargetVariable]\n",
    "y = np.ravel(df.loc[:, df.columns == TargetVariable])\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the string target class values (\"regular\" or \"lapsed\") as integers in a numpy array\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(y)\n",
    "label_encoded_y = label_encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into 70% training, 30% test, using the encoded numpy array\n",
    "seed = 7\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label_encoded_y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "Untuned accuracy: 78.60%\n",
      "--- 98.3970000743866 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Approach 1: call XGBClassifier with no parameters, using all of the default values, to measure the baseline accuracy\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model to training data\n",
    "model_baseline = xgb.XGBClassifier()\n",
    "model_baseline.fit(X_train, y_train)\n",
    "print(model_baseline)\n",
    "\n",
    "# Make predictions for the test data\n",
    "y_pred = model_baseline.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate the predictions made using the test data\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Untuned accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Save the model to disk using joblib’s replacement of pickle (dump & load), which is more efficient on objects\n",
    "# that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators\n",
    "dump(model_baseline, 'model_baseline.joblib')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Output:\n",
    "#     XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#                   colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "#                   learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "#                   min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
    "#                   nthread=None, objective='binary:logistic', random_state=0,\n",
    "#                   reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "#                   silent=None, subsample=1, verbosity=1)\n",
    "#     Untuned accuracy: 79.00%\n",
    "#     --- 90.91621375083923 seconds ---\n",
    "# So, our baseline accuracy on the test dataset is 79.00% - can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuned cross-validated accuracy: 78.99%\n",
      "Untuned cross-validated AUC: 0.7869\n",
      "--- 42.51188898086548 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_cv.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 2: See if we can improve this using XGBoost's built-in cross-validation capabilities\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the DMatrix required for the xgboost cv method, using some minimal parameters\n",
    "dmatrix = xgb.DMatrix(data = X_train, label = y_train)\n",
    "params={\"objective\":\"binary:logistic\",\"max_depth\":4}\n",
    "model_cv = xgb.cv(dtrain = dmatrix, params = params, nfold = 4, num_boost_round = 10, metrics = [\"error\",\"auc\"],\n",
    "                  as_pandas = True)\n",
    "\n",
    "print(\"Untuned cross-validated accuracy: %.2f%%\" %(((1 - model_cv[\"test-error-mean\"]).iloc[-1]) * 100.0))\n",
    "print(\"Untuned cross-validated AUC: %.4f\" %(((model_cv[\"test-auc-mean\"]).iloc[-1])))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_cv, 'model_cv.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Untuned cross-validated accuracy: 78.85%\n",
    "#     Untuned cross-validated AUC: 0.7878\n",
    "#     --- 41.71943497657776 seconds ---\n",
    "# This accuracy of 78.85% is slightly lower than the baseline accuracy of 79.00%.\n",
    "# So, cross-validation on its own is not going to \"magically\" improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated early_stopping accuracy: 79.10%\n",
      "Cross-validated early_stopping AUC: 0.7943\n",
      "--- 137.35042786598206 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_cv_early_stopping.joblib']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 3: See if we can improve this using a higher number of boosting rounds\n",
    "# with automated boosting round selection using early_stopping\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the DMatrix required for the xgboost cv method, using some minimal parameters\n",
    "dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
    "params = {\"objective\":\"binary:logistic\", \"max_depth\":4}\n",
    "model_cv_early_stopping = xgb.cv(dtrain = dmatrix, params = params, early_stopping_rounds = 10, nfold = 4,\n",
    "                                 num_boost_round = 50, metrics = [\"error\",\"auc\"], seed = 0, as_pandas = True)\n",
    "\n",
    "print(\"Cross-validated early_stopping accuracy: %.2f%%\" %(((1 - model_cv_early_stopping[\"test-error-mean\"]).iloc[-1]) * 100.0))\n",
    "print(\"Cross-validated early_stopping AUC: %.4f\" %(((model_cv_early_stopping[\"test-auc-mean\"]).iloc[-1])))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_cv_early_stopping, 'model_cv_early_stopping.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Cross-validated early_stopping accuracy: 79.10%\n",
    "#     Cross-validated early_stopping AUC: 0.7943\n",
    "#     --- 137.35042786598206 seconds ---\n",
    "# This accuracy of 79.10% is slightly higher than the baseline accuracy of 79.00%,\n",
    "# and the AUC of 0.7943 is also slightly higher than the untuned cross-validated AUC of 0.7878.\n",
    "# However, we're not getting much improvement, so time to start tuning the XGBoost hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For my reference: some frequently tuned XGBoost parameters for the tree base learner:\n",
    "#\n",
    "# learning rate (eta):\n",
    "#     Affects how quickly the model fits the residual error using additional base learners.\n",
    "#     A low learning rate will require more boosting rounds to achieve the same reduction in residual error as an\n",
    "#     XGBoost model with a high learning rate.\n",
    "# gamma:\n",
    "#     Minimum loss reduction to create new tree split, which affects how strongly regularised the trained model will\n",
    "#     be.\n",
    "# lambda:\n",
    "#     L2 regularisation on leaf weights, which affects how strongly regularised the trained model will be.\n",
    "# alpha:\n",
    "#     L1 regularisation on leaf weights, which affects how strongly regularised the trained model will be.\n",
    "# max_depth:\n",
    "#     Maximum depth per tree.\n",
    "#     Must be a positive integer value.\n",
    "#     Affects how deeply each tree is allowed to grow during any given boosting round.\n",
    "# subsample:\n",
    "#     Percentage of samples used per tree.\n",
    "#     Must be a value between 0 and 1.\n",
    "#     The fraction of the total training set that can be used for any given boosting round.\n",
    "#     If the value is low, then the fraction of training data used per boosting round would be low and may run into\n",
    "#     underfitting problems, while a value that is very high can lead to overfitting.\n",
    "# colsample_bytree:\n",
    "#     Percentage of features used per tree.\n",
    "#     The fraction of features that can be selected from during any given boosting round.\n",
    "#     Must be a value between 0 and 1.\n",
    "#     A large value means that almost all features can be used to build a tree during a given boosting round,\n",
    "#     while a small value means that the fraction of features that can be selected from is very small.\n",
    "#     Smaller values can be thought of as providing additional regularisation to the model,\n",
    "#     while using all columns may overfit a trained model.\n",
    "# num_boost_round:\n",
    "#     Number of boosting rounds.\n",
    "#     The number of trees to be built or the number of base learners to be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n",
      "Best AUC found: 0.7955\n",
      "--- 155.17163109779358 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_grid_search.joblib']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 4: See if we can improve this by tuning a few of the above hyperparameters using a very simple GridSearch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    \"colsample_bytree\": [0.3, 0.7],\n",
    "    \"n_estimators\": [50],\n",
    "    \"max_depth\": [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the classifier\n",
    "gbm = xgb.XGBClassifier()\n",
    "\n",
    "# Perform grid search\n",
    "model_grid_search = GridSearchCV(param_grid = gbm_param_grid, estimator = gbm, scoring = \"roc_auc\", cv = 4,\n",
    "                                 n_jobs = -1, verbose = 1)\n",
    "\n",
    "# Fit model_grid_search to the data\n",
    "model_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", model_grid_search.best_params_)\n",
    "print(\"Best AUC found: %.4f\" %model_grid_search.best_score_)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_grid_search, 'model_grid_search.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
    "#     [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
    "#     [Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:  1.7min finished\n",
    "#     Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\n",
    "#     Best AUC found: 0.7955\n",
    "#     --- 155.17163109779358 seconds ---\n",
    "# This AUC of 0.7955 is slightly better than the cross-validated early_stopping AUC of 0.7943.\n",
    "# However, we just picked some hyperparameters somewhat haphazardly, so let's try a more structured approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create XGBoost models and perform cross-validation\n",
    "def modelfit(alg, data, label, useTrainCV = True, cv_folds = 5, early_stopping_rounds = 50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(data = data, label = label)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round = alg.get_params()['n_estimators'], nfold = cv_folds,\n",
    "            metrics = 'auc', early_stopping_rounds = early_stopping_rounds)\n",
    "        alg.set_params(n_estimators = cvresult.shape[0])\n",
    "    \n",
    "    # Fit the algorithm on the data\n",
    "    alg.fit(data, label, eval_metric = 'auc', verbose = True)\n",
    "        \n",
    "    # Predict training set\n",
    "    dtrain_predictions = alg.predict(data)\n",
    "    dtrain_predprob = alg.predict_proba(data)[:,1]\n",
    "        \n",
    "    # Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(label, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %.4f\" % metrics.roc_auc_score(label, dtrain_predprob))\n",
    "    print(\"Best Iteration: {}\".format(alg.get_booster().best_iteration))  \n",
    "    print(\"Best ntree limit: {}\".format(alg.get_booster().best_ntree_limit))\n",
    "    \n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore()).nlargest(50).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind = 'bar', title = 'Top 50 Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8053\n",
      "AUC Score (Train): 0.8213\n",
      "Best Iteration: 128\n",
      "Best ntree limit: 129\n",
      "--- 265.79448223114014 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_1.joblib']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approach 5: Try a step-by-step approach to hyperparameter tuning\n",
    "# based on https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Fix the learning rate and the number of estimators, with typical values for other parameters\n",
    "model_step_by_step_1 = \\\n",
    "    xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 1000, max_depth = 5, min_child_weight = 1,\n",
    "                      gamma = 0, subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic',\n",
    "                      nthread = 4, scale_pos_weight = 1, seed = 27)\n",
    "\n",
    "modelfit(model_step_by_step_1, X_train, y_train)\n",
    "\n",
    "# Save the initial Feature Importances bar chart to a file\n",
    "plt.savefig('Feature Importances - initial.png')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_1, 'model_step_by_step_1.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Model Report\n",
    "#     Accuracy : 0.8053\n",
    "#     AUC Score (Train): 0.8213\n",
    "#     Best Iteration: 128\n",
    "#     Best ntree limit: 129\n",
    "#     --- 265.79448223114014 seconds ---\n",
    "# So, we've improved the AUC from 0.7955 to 0.8213. Let's keep tuning to see if we can improve this further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The initial Feature Importances bar chart can be viewed [here](https://github.com/DavidSchanzer823239622/UTS_ML2019_82329622/blob/master/Feature%20Importances%20-%20initial.png \"Initial Feature Importances bar chart\"))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed: 28.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_test_score  std_test_score                                   params\n",
      "0          0.792775        0.002397  {'max_depth': 3, 'min_child_weight': 1}\n",
      "1          0.792989        0.002257  {'max_depth': 3, 'min_child_weight': 3}\n",
      "2          0.792919        0.002463  {'max_depth': 3, 'min_child_weight': 5}\n",
      "3          0.797768        0.002121  {'max_depth': 5, 'min_child_weight': 1}\n",
      "4          0.797900        0.002359  {'max_depth': 5, 'min_child_weight': 3}\n",
      "5          0.798228        0.002526  {'max_depth': 5, 'min_child_weight': 5}\n",
      "6          0.798096        0.002827  {'max_depth': 7, 'min_child_weight': 1}\n",
      "7          0.798151        0.002446  {'max_depth': 7, 'min_child_weight': 3}\n",
      "8          0.798359        0.002328  {'max_depth': 7, 'min_child_weight': 5}\n",
      "9          0.795148        0.001951  {'max_depth': 9, 'min_child_weight': 1}\n",
      "10         0.795467        0.001924  {'max_depth': 9, 'min_child_weight': 3}\n",
      "11         0.796833        0.001508  {'max_depth': 9, 'min_child_weight': 5}\n",
      "Best params: {'max_depth': 7, 'min_child_weight': 5}\n",
      "Best score: 0.798359\n",
      "--- 1784.0705111026764 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_2.joblib']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Tune max_depth and min_child_weight, as they will have the highest impact on model outcome.\n",
    "# Use the optimal n_estimators value calculated in the previous step (129).\n",
    "# To start with, set wider ranges for max_depth and min_child_weight and then perform another iteration for\n",
    "# smaller ranges.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "param_step_by_step_2 = {\n",
    "    'max_depth': range(3, 10, 2),        # Values [3, 5, 7, 9]\n",
    "    'min_child_weight': range(1, 6, 2)   # Values [1, 3, 5]\n",
    "}\n",
    "model_step_by_step_2 = \\\n",
    "    GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 129, max_depth = 5,\n",
    "                 min_child_weight = 1, gamma = 0, subsample = 0.8, colsample_bytree = 0.8,\n",
    "                 objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27),\n",
    "                 param_grid = param_step_by_step_2, scoring = 'roc_auc',n_jobs = 4, iid = False, cv = 5,\n",
    "                 verbose = 1)\n",
    "    \n",
    "model_step_by_step_2.fit(X_train, y_train)\n",
    "\n",
    "print(pd.DataFrame(model_step_by_step_2.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(\"Best params: %s\" % model_step_by_step_2.best_params_)\n",
    "print(\"Best score: %f\" % model_step_by_step_2.best_score_)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_2, 'model_step_by_step_2.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
    "#     [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "#     [Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 18.4min\n",
    "#     [Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed: 28.9min finished\n",
    "#         mean_test_score  std_test_score                                   params\n",
    "#     0          0.792775        0.002397  {'max_depth': 3, 'min_child_weight': 1}\n",
    "#     1          0.792989        0.002257  {'max_depth': 3, 'min_child_weight': 3}\n",
    "#     2          0.792919        0.002463  {'max_depth': 3, 'min_child_weight': 5}\n",
    "#     3          0.797768        0.002121  {'max_depth': 5, 'min_child_weight': 1}\n",
    "#     4          0.797900        0.002359  {'max_depth': 5, 'min_child_weight': 3}\n",
    "#     5          0.798228        0.002526  {'max_depth': 5, 'min_child_weight': 5}\n",
    "#     6          0.798096        0.002827  {'max_depth': 7, 'min_child_weight': 1}\n",
    "#     7          0.798151        0.002446  {'max_depth': 7, 'min_child_weight': 3}\n",
    "#     8          0.798359        0.002328  {'max_depth': 7, 'min_child_weight': 5}\n",
    "#     9          0.795148        0.001951  {'max_depth': 9, 'min_child_weight': 1}\n",
    "#     10         0.795467        0.001924  {'max_depth': 9, 'min_child_weight': 3}\n",
    "#     11         0.796833        0.001508  {'max_depth': 9, 'min_child_weight': 5}\n",
    "#     Best params: {'max_depth': 7, 'min_child_weight': 5}\n",
    "#     Best score: 0.798359\n",
    "#     --- 1784.0705111026764 seconds ---\n",
    "# So, the best values discovered so far (using an interval of two) are 7 for max_depth and 5 for min_child_weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed: 23.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_test_score  std_test_score                                   params\n",
      "0         0.798927        0.002129  {'max_depth': 6, 'min_child_weight': 4}\n",
      "1         0.798543        0.002475  {'max_depth': 6, 'min_child_weight': 5}\n",
      "2         0.798449        0.002379  {'max_depth': 6, 'min_child_weight': 6}\n",
      "3         0.798134        0.002382  {'max_depth': 7, 'min_child_weight': 4}\n",
      "4         0.798359        0.002328  {'max_depth': 7, 'min_child_weight': 5}\n",
      "5         0.798307        0.002430  {'max_depth': 7, 'min_child_weight': 6}\n",
      "6         0.797472        0.002018  {'max_depth': 8, 'min_child_weight': 4}\n",
      "7         0.797375        0.002247  {'max_depth': 8, 'min_child_weight': 5}\n",
      "8         0.797992        0.002273  {'max_depth': 8, 'min_child_weight': 6}\n",
      "Best params: {'max_depth': 6, 'min_child_weight': 4}\n",
      "Best score: 0.798927\n",
      "--- 1472.9096410274506 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_3.joblib']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Fine-tune max_depth and min_child_weight, looking for optimum values,\n",
    "# by searching for values 1 above and below the best values discovered so far.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "param_step_by_step_3 = {\n",
    "    'max_depth': [6, 7, 8],\n",
    "    'min_child_weight': [4, 5, 6]\n",
    "}\n",
    "\n",
    "model_step_by_step_3 = \\\n",
    "    GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 129, max_depth = 7,\n",
    "                 min_child_weight = 5, gamma = 0, subsample = 0.8, colsample_bytree = 0.8,\n",
    "                 objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27), \n",
    "                 param_grid = param_step_by_step_3, scoring = 'roc_auc', n_jobs = 4, iid = False, cv = 5,\n",
    "                 verbose = 1)\n",
    "\n",
    "model_step_by_step_3.fit(X_train, y_train)\n",
    "\n",
    "print(pd.DataFrame(model_step_by_step_3.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(\"Best params: %s\" % model_step_by_step_3.best_params_)\n",
    "print(\"Best score: %f\" % model_step_by_step_3.best_score_)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_3, 'model_step_by_step_3.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
    "#     [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "#     [Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed: 23.8min finished\n",
    "#        mean_test_score  std_test_score                                   params\n",
    "#     0         0.798927        0.002129  {'max_depth': 6, 'min_child_weight': 4}\n",
    "#     1         0.798543        0.002475  {'max_depth': 6, 'min_child_weight': 5}\n",
    "#     2         0.798449        0.002379  {'max_depth': 6, 'min_child_weight': 6}\n",
    "#     3         0.798134        0.002382  {'max_depth': 7, 'min_child_weight': 4}\n",
    "#     4         0.798359        0.002328  {'max_depth': 7, 'min_child_weight': 5}\n",
    "#     5         0.798307        0.002430  {'max_depth': 7, 'min_child_weight': 6}\n",
    "#     6         0.797472        0.002018  {'max_depth': 8, 'min_child_weight': 4}\n",
    "#     7         0.797375        0.002247  {'max_depth': 8, 'min_child_weight': 5}\n",
    "#     8         0.797992        0.002273  {'max_depth': 8, 'min_child_weight': 6}\n",
    "#     Best params: {'max_depth': 6, 'min_child_weight': 4}\n",
    "#     Best score: 0.798927\n",
    "#     --- 1472.9096410274506 seconds ---\n",
    "# So, we've found that the optimum values for max_depth is 6, and for min_child_weight is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed: 11.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_test_score  std_test_score          params\n",
      "0         0.798927        0.002129  {'gamma': 0.0}\n",
      "1         0.798545        0.002114  {'gamma': 0.1}\n",
      "2         0.798390        0.002208  {'gamma': 0.2}\n",
      "3         0.798590        0.002178  {'gamma': 0.3}\n",
      "4         0.798611        0.002357  {'gamma': 0.4}\n",
      "Best params: {'gamma': 0.0}\n",
      "Best score: 0.798927\n",
      "--- 752.3053398132324 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_4.joblib']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Tune gamma using the parameters already tuned above.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "param_step_by_step_4 = {\n",
    "    'gamma': [i/10.0 for i in range(0,5)]    # Values [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "model_step_by_step_4 = \\\n",
    "    GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 129, max_depth = 6,\n",
    "                 min_child_weight = 4, gamma = 0, subsample = 0.8, colsample_bytree = 0.8,\n",
    "                 objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27), \n",
    "                 param_grid = param_step_by_step_4, scoring = 'roc_auc', n_jobs = 4, iid = False, cv = 5,\n",
    "                 verbose = 1)\n",
    "\n",
    "model_step_by_step_4.fit(X_train, y_train)\n",
    "\n",
    "print(pd.DataFrame(model_step_by_step_4.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(\"Best params: %s\" % model_step_by_step_4.best_params_)\n",
    "print(\"Best score: %f\" % model_step_by_step_4.best_score_)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_4, 'model_step_by_step_4.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
    "#     [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "#     [Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed: 11.8min finished\n",
    "#        mean_test_score  std_test_score          params\n",
    "#     0         0.798927        0.002129  {'gamma': 0.0}\n",
    "#     1         0.798545        0.002114  {'gamma': 0.1}\n",
    "#     2         0.798390        0.002208  {'gamma': 0.2}\n",
    "#     3         0.798590        0.002178  {'gamma': 0.3}\n",
    "#     4         0.798611        0.002357  {'gamma': 0.4}\n",
    "#     Best params: {'gamma': 0.0}\n",
    "#     Best score: 0.798927\n",
    "#     --- 752.3053398132324 seconds ---\n",
    "# So, the optimum value for gamma, given the previous parameters, is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8114\n",
      "AUC Score (Train): 0.8316\n",
      "Best Iteration: 128\n",
      "Best ntree limit: 129\n",
      "--- 234.36470699310303 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_5.joblib']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Re-calibrate the number of boosting rounds for the updated parameters\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_step_by_step_5 = \\\n",
    "    xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 129, max_depth = 6, min_child_weight = 4, gamma = 0,\n",
    "                  subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic', nthread = 4,\n",
    "                  scale_pos_weight = 1, seed = 27)\n",
    "\n",
    "modelfit(model_step_by_step_5, X_train, y_train)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_5, 'model_step_by_step_5.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Model Report\n",
    "#     Accuracy : 0.8114\n",
    "#     AUC Score (Train): 0.8316\n",
    "#     Best Iteration: 128\n",
    "#     Best ntree limit: 129\n",
    "#     --- 234.36470699310303 seconds ---\n",
    "# So, with max_depth = 6, min_child_weight = 4 and gamma= 0, the optimum n_estimators value is still 129."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=4)]: Done  80 out of  80 | elapsed: 36.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_test_score  std_test_score  \\\n",
      "0          0.797229        0.002333   \n",
      "1          0.797729        0.002164   \n",
      "2          0.798429        0.002594   \n",
      "3          0.798609        0.002403   \n",
      "4          0.798080        0.002859   \n",
      "5          0.798242        0.002294   \n",
      "6          0.798585        0.002031   \n",
      "7          0.798720        0.001836   \n",
      "8          0.797463        0.002507   \n",
      "9          0.798609        0.002218   \n",
      "10         0.798927        0.002129   \n",
      "11         0.799125        0.002065   \n",
      "12         0.797627        0.002651   \n",
      "13         0.797927        0.002148   \n",
      "14         0.798310        0.002229   \n",
      "15         0.799003        0.001883   \n",
      "\n",
      "                                         params  \n",
      "0   {'colsample_bytree': 0.6, 'subsample': 0.6}  \n",
      "1   {'colsample_bytree': 0.6, 'subsample': 0.7}  \n",
      "2   {'colsample_bytree': 0.6, 'subsample': 0.8}  \n",
      "3   {'colsample_bytree': 0.6, 'subsample': 0.9}  \n",
      "4   {'colsample_bytree': 0.7, 'subsample': 0.6}  \n",
      "5   {'colsample_bytree': 0.7, 'subsample': 0.7}  \n",
      "6   {'colsample_bytree': 0.7, 'subsample': 0.8}  \n",
      "7   {'colsample_bytree': 0.7, 'subsample': 0.9}  \n",
      "8   {'colsample_bytree': 0.8, 'subsample': 0.6}  \n",
      "9   {'colsample_bytree': 0.8, 'subsample': 0.7}  \n",
      "10  {'colsample_bytree': 0.8, 'subsample': 0.8}  \n",
      "11  {'colsample_bytree': 0.8, 'subsample': 0.9}  \n",
      "12  {'colsample_bytree': 0.9, 'subsample': 0.6}  \n",
      "13  {'colsample_bytree': 0.9, 'subsample': 0.7}  \n",
      "14  {'colsample_bytree': 0.9, 'subsample': 0.8}  \n",
      "15  {'colsample_bytree': 0.9, 'subsample': 0.9}  \n",
      "Best params: {'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "Best score: 0.799125\n",
      "--- 2207.8702890872955 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_6.joblib']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Tune subsample and colsample_bytree\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "param_step_by_step_6 = {\n",
    "    'subsample': [i/10.0 for i in range(6,10)],         # Values [0.6, 0.7, 0.8, 0.9]\n",
    "    'colsample_bytree': [i/10.0 for i in range(6,10)]   # Values [0.6, 0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "model_step_by_step_6 = \\\n",
    "    GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 129, max_depth = 6,\n",
    "                 min_child_weight = 4, gamma = 0, subsample = 0.8, colsample_bytree = 0.8,\n",
    "                 objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27), \n",
    "                 param_grid = param_step_by_step_6, scoring = 'roc_auc', n_jobs = 4, iid = False, cv = 5,\n",
    "                 verbose = 1)\n",
    "\n",
    "model_step_by_step_6.fit(X_train, y_train)\n",
    "\n",
    "print(pd.DataFrame(model_step_by_step_6.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(\"Best params: %s\" % model_step_by_step_6.best_params_)\n",
    "print(\"Best score: %f\" % model_step_by_step_6.best_score_)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_6, 'model_step_by_step_6.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
    "#     [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "#     [Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 18.5min\n",
    "#     [Parallel(n_jobs=4)]: Done  80 out of  80 | elapsed: 36.0min finished\n",
    "#         mean_test_score  std_test_score  \\\n",
    "#     0          0.797229        0.002333   \n",
    "#     1          0.797729        0.002164   \n",
    "#     2          0.798429        0.002594   \n",
    "#     3          0.798609        0.002403   \n",
    "#     4          0.798080        0.002859   \n",
    "#     5          0.798242        0.002294   \n",
    "#     6          0.798585        0.002031   \n",
    "#     7          0.798720        0.001836   \n",
    "#     8          0.797463        0.002507   \n",
    "#     9          0.798609        0.002218   \n",
    "#     10         0.798927        0.002129   \n",
    "#     11         0.799125        0.002065   \n",
    "#     12         0.797627        0.002651   \n",
    "#     13         0.797927        0.002148   \n",
    "#     14         0.798310        0.002229   \n",
    "#     15         0.799003        0.001883   \n",
    "#     \n",
    "#                                              params  \n",
    "#     0   {'colsample_bytree': 0.6, 'subsample': 0.6}  \n",
    "#     1   {'colsample_bytree': 0.6, 'subsample': 0.7}  \n",
    "#     2   {'colsample_bytree': 0.6, 'subsample': 0.8}  \n",
    "#     3   {'colsample_bytree': 0.6, 'subsample': 0.9}  \n",
    "#     4   {'colsample_bytree': 0.7, 'subsample': 0.6}  \n",
    "#     5   {'colsample_bytree': 0.7, 'subsample': 0.7}  \n",
    "#     6   {'colsample_bytree': 0.7, 'subsample': 0.8}  \n",
    "#     7   {'colsample_bytree': 0.7, 'subsample': 0.9}  \n",
    "#     8   {'colsample_bytree': 0.8, 'subsample': 0.6}  \n",
    "#     9   {'colsample_bytree': 0.8, 'subsample': 0.7}  \n",
    "#     10  {'colsample_bytree': 0.8, 'subsample': 0.8}  \n",
    "#     11  {'colsample_bytree': 0.8, 'subsample': 0.9}  \n",
    "#     12  {'colsample_bytree': 0.9, 'subsample': 0.6}  \n",
    "#     13  {'colsample_bytree': 0.9, 'subsample': 0.7}  \n",
    "#     14  {'colsample_bytree': 0.9, 'subsample': 0.8}  \n",
    "#     15  {'colsample_bytree': 0.9, 'subsample': 0.9}  \n",
    "#     Best params: {'colsample_bytree': 0.8, 'subsample': 0.9}\n",
    "#     Best score: 0.799125\n",
    "#     --- 2207.8702890872955 seconds ---\n",
    "# So, given the above parameters, the best value so far for subsample is 0.9 and for colsample_bytree was 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed: 21.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_test_score  std_test_score  \\\n",
      "0         0.798669        0.002629   \n",
      "1         0.799158        0.002359   \n",
      "2         0.799457        0.002496   \n",
      "3         0.799124        0.002814   \n",
      "4         0.799125        0.002065   \n",
      "5         0.799129        0.002119   \n",
      "6         0.798629        0.002213   \n",
      "7         0.798867        0.002451   \n",
      "8         0.798500        0.002351   \n",
      "\n",
      "                                          params  \n",
      "0  {'colsample_bytree': 0.75, 'subsample': 0.85}  \n",
      "1   {'colsample_bytree': 0.75, 'subsample': 0.9}  \n",
      "2  {'colsample_bytree': 0.75, 'subsample': 0.95}  \n",
      "3   {'colsample_bytree': 0.8, 'subsample': 0.85}  \n",
      "4    {'colsample_bytree': 0.8, 'subsample': 0.9}  \n",
      "5   {'colsample_bytree': 0.8, 'subsample': 0.95}  \n",
      "6  {'colsample_bytree': 0.85, 'subsample': 0.85}  \n",
      "7   {'colsample_bytree': 0.85, 'subsample': 0.9}  \n",
      "8  {'colsample_bytree': 0.85, 'subsample': 0.95}  \n",
      "Best params: {'colsample_bytree': 0.75, 'subsample': 0.95}\n",
      "Best score: 0.799457\n",
      "--- 1308.6555030345917 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_7.joblib']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: The best value so far for subsample was 0.9, and for colsample_bytree was 0.8.\n",
    "# Now try values in 0.05 intervals around these.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "param_step_by_step_7 = {\n",
    "    'subsample': [i/100.0 for i in range(85,100,5)],        # Values [0.85, 0.9, 0.95]\n",
    "    'colsample_bytree': [i/100.0 for i in range(75,90,5)]   # Values [0.75, 0.8, 0.85]\n",
    "}\n",
    "\n",
    "model_step_by_step_7 = \\\n",
    "    GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 129, max_depth = 6,\n",
    "                 min_child_weight = 4, gamma = 0, subsample = 0.9, colsample_bytree = 0.8,\n",
    "                 objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27), \n",
    "                 param_grid = param_step_by_step_7, scoring = 'roc_auc', n_jobs = 4, iid = False, cv = 5,\n",
    "                 verbose = 1)\n",
    "\n",
    "model_step_by_step_7.fit(X_train, y_train)\n",
    "\n",
    "print(pd.DataFrame(model_step_by_step_7.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(\"Best params: %s\" % model_step_by_step_7.best_params_)\n",
    "print(\"Best score: %f\" % model_step_by_step_7.best_score_)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_7, 'model_step_by_step_7.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
    "#     [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "#     [Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed: 21.1min finished\n",
    "#        mean_test_score  std_test_score  \\\n",
    "#     0         0.798669        0.002629   \n",
    "#     1         0.799158        0.002359   \n",
    "#     2         0.799457        0.002496   \n",
    "#     3         0.799124        0.002814   \n",
    "#     4         0.799125        0.002065   \n",
    "#     5         0.799129        0.002119   \n",
    "#     6         0.798629        0.002213   \n",
    "#     7         0.798867        0.002451   \n",
    "#     8         0.798500        0.002351   \n",
    "#     \n",
    "#                                               params  \n",
    "#     0  {'colsample_bytree': 0.75, 'subsample': 0.85}  \n",
    "#     1   {'colsample_bytree': 0.75, 'subsample': 0.9}  \n",
    "#     2  {'colsample_bytree': 0.75, 'subsample': 0.95}  \n",
    "#     3   {'colsample_bytree': 0.8, 'subsample': 0.85}  \n",
    "#     4    {'colsample_bytree': 0.8, 'subsample': 0.9}  \n",
    "#     5   {'colsample_bytree': 0.8, 'subsample': 0.95}  \n",
    "#     6  {'colsample_bytree': 0.85, 'subsample': 0.85}  \n",
    "#     7   {'colsample_bytree': 0.85, 'subsample': 0.9}  \n",
    "#     8  {'colsample_bytree': 0.85, 'subsample': 0.95}  \n",
    "#     Best params: {'colsample_bytree': 0.75, 'subsample': 0.95}\n",
    "#     Best score: 0.799457\n",
    "#     --- 1308.6555030345917 seconds ---\n",
    "# So, the optimum value for subsample has been refined to 0.95, and for colsample_bytree to 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_test_score  std_test_score                params\n",
      "0         0.799457        0.002496  {'reg_alpha': 1e-05}\n",
      "1         0.799444        0.002456   {'reg_alpha': 0.01}\n",
      "2         0.799148        0.002625    {'reg_alpha': 0.1}\n",
      "3         0.799720        0.002136      {'reg_alpha': 1}\n",
      "4         0.791205        0.001887    {'reg_alpha': 100}\n",
      "Best params: {'reg_alpha': 1}\n",
      "Best score: 0.799720\n",
      "--- 705.3381948471069 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_8.joblib']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Apply regularization to reduce overfitting, by tuning reg_alpha.\n",
    "# Tuning reg_lambda may also be useful to further reduce overfitting, but the decision was made that this is enough\n",
    "# tuning for the purpose of this assignment.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "param_step_by_step_8 = {\n",
    "    'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "model_step_by_step_8 = \\\n",
    "    GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 129, max_depth = 6,\n",
    "                 min_child_weight = 4, gamma = 0, subsample = 0.95, colsample_bytree = 0.75,\n",
    "                 objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27), \n",
    "                 param_grid = param_step_by_step_8, scoring = 'roc_auc', n_jobs = 4, iid = False, cv = 5,\n",
    "                 verbose = 1)\n",
    "\n",
    "model_step_by_step_8.fit(X_train, y_train)\n",
    "\n",
    "print(pd.DataFrame(model_step_by_step_8.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(\"Best params: %s\" % model_step_by_step_8.best_params_)\n",
    "print(\"Best score: %f\" % model_step_by_step_8.best_score_)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_8, 'model_step_by_step_8.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
    "#     [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "#     [Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed: 11.0min finished\n",
    "#        mean_test_score  std_test_score                params\n",
    "#     0         0.799457        0.002496  {'reg_alpha': 1e-05}\n",
    "#     1         0.799444        0.002456   {'reg_alpha': 0.01}\n",
    "#     2         0.799148        0.002625    {'reg_alpha': 0.1}\n",
    "#     3         0.799720        0.002136      {'reg_alpha': 1}\n",
    "#     4         0.791205        0.001887    {'reg_alpha': 100}\n",
    "#     Best params: {'reg_alpha': 1}\n",
    "#     Best score: 0.799720\n",
    "#     --- 705.3381948471069 seconds ---\n",
    "# So, the best value so far for reg_alpha from these 5 possible values is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 18.3min\n",
      "[Parallel(n_jobs=4)]: Done  70 out of  70 | elapsed: 29.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_test_score  std_test_score              params\n",
      "0          0.799333        0.002465  {'reg_alpha': 0.5}\n",
      "1          0.799116        0.002172    {'reg_alpha': 1}\n",
      "2          0.800166        0.002031    {'reg_alpha': 5}\n",
      "3          0.800270        0.002156    {'reg_alpha': 6}\n",
      "4          0.800363        0.002301    {'reg_alpha': 7}\n",
      "5          0.800188        0.002046    {'reg_alpha': 8}\n",
      "6          0.800091        0.002142    {'reg_alpha': 9}\n",
      "7          0.799958        0.002240   {'reg_alpha': 10}\n",
      "8          0.799878        0.002038   {'reg_alpha': 11}\n",
      "9          0.799791        0.002194   {'reg_alpha': 12}\n",
      "10         0.800000        0.002170   {'reg_alpha': 13}\n",
      "11         0.799666        0.002205   {'reg_alpha': 14}\n",
      "12         0.799583        0.001937   {'reg_alpha': 15}\n",
      "13         0.798932        0.001976   {'reg_alpha': 20}\n",
      "Best params: {'reg_alpha': 7}\n",
      "Best score: 0.800363\n",
      "--- 1795.949402809143 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_9.joblib']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 9: Try values of reg_alpha closer to the best value so far (1) to see if we get something better.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "param_step_by_step_9 = {\n",
    "    'reg_alpha': [0.5, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20]\n",
    "}\n",
    "\n",
    "model_step_by_step_9 = \\\n",
    "    GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 129, max_depth = 6,\n",
    "                 min_child_weight = 4, gamma = 0, subsample = 0.95, colsample_bytree = 0.65,\n",
    "                 objective = 'binary:logistic', nthread = 4, scale_pos_weight = 1, seed = 27), \n",
    "                 param_grid = param_step_by_step_9, scoring = 'roc_auc', n_jobs = 4, iid = False, cv = 5,\n",
    "                 verbose = 1)\n",
    "\n",
    "model_step_by_step_9.fit(X_train, y_train)\n",
    "\n",
    "print(pd.DataFrame(model_step_by_step_9.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(\"Best params: %s\" % model_step_by_step_9.best_params_)\n",
    "print(\"Best score: %f\" % model_step_by_step_9.best_score_)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_9, 'model_step_by_step_9.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
    "#     [Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "#     [Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 18.3min\n",
    "#     [Parallel(n_jobs=4)]: Done  70 out of  70 | elapsed: 29.3min finished\n",
    "#         mean_test_score  std_test_score              params\n",
    "#     0          0.799333        0.002465  {'reg_alpha': 0.5}\n",
    "#     1          0.799116        0.002172    {'reg_alpha': 1}\n",
    "#     2          0.800166        0.002031    {'reg_alpha': 5}\n",
    "#     3          0.800270        0.002156    {'reg_alpha': 6}\n",
    "#     4          0.800363        0.002301    {'reg_alpha': 7}\n",
    "#     5          0.800188        0.002046    {'reg_alpha': 8}\n",
    "#     6          0.800091        0.002142    {'reg_alpha': 9}\n",
    "#     7          0.799958        0.002240   {'reg_alpha': 10}\n",
    "#     8          0.799878        0.002038   {'reg_alpha': 11}\n",
    "#     9          0.799791        0.002194   {'reg_alpha': 12}\n",
    "#     10         0.800000        0.002170   {'reg_alpha': 13}\n",
    "#     11         0.799666        0.002205   {'reg_alpha': 14}\n",
    "#     12         0.799583        0.001937   {'reg_alpha': 15}\n",
    "#     13         0.798932        0.001976   {'reg_alpha': 20}\n",
    "#     Best params: {'reg_alpha': 7}\n",
    "#     Best score: 0.800363\n",
    "#     --- 1795.949402809143 seconds ---\n",
    "#     # So, given the above parameters, the optimum value for reg_alpha is 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8114\n",
      "AUC Score (Train): 0.8319\n",
      "Best Iteration: 203\n",
      "Best ntree limit: 204\n",
      "--- 417.5869870185852 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_10.joblib']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 10: Apply this regularization (reg_alpha) in the model and look at the impact.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_step_by_step_10 = \\\n",
    "    xgb.XGBClassifier(learning_rate = 0.1, n_estimators = 1000, max_depth = 6, min_child_weight = 4, gamma = 0,\n",
    "                      subsample = 0.95, colsample_bytree = 0.65, reg_alpha = 7, objective = 'binary:logistic',\n",
    "                      nthread = 4, scale_pos_weight = 1, seed = 27)\n",
    "\n",
    "modelfit(model_step_by_step_10, X_train, y_train)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_10, 'model_step_by_step_10.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Model Report\n",
    "#     Accuracy : 0.8114\n",
    "#     AUC Score (Train): 0.8319\n",
    "#     Best Iteration: 203\n",
    "#     Best ntree limit: 204\n",
    "#     --- 417.5869870185852 seconds ---\n",
    "# So, the new best AUC is 0.8319, which is achieved with an n_estimators value of 204."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8092\n",
      "AUC Score (Train): 0.8266\n",
      "Best Iteration: 1602\n",
      "Best ntree limit: 1603\n",
      "--- 2614.470659017563 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_step_by_step_11.joblib']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 11: Finally, lower the learning rate and add more trees, using the cv function of XGBoost.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_step_by_step_11 = \\\n",
    "    xgb.XGBClassifier(learning_rate = 0.01, n_estimators = 5000, max_depth = 6, min_child_weight = 4, gamma = 0,\n",
    "                      subsample = 0.95, colsample_bytree = 0.65, reg_alpha = 7, objective = 'binary:logistic',\n",
    "                      nthread = 4, scale_pos_weight = 1, seed = 27)\n",
    "\n",
    "modelfit(model_step_by_step_11, X_train, y_train)\n",
    "\n",
    "# Save the final Feature Importances bar chart to a file\n",
    "plt.savefig('Feature Importances - final.png')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save the model to disk\n",
    "dump(model_step_by_step_11, 'model_step_by_step_11.joblib')\n",
    "\n",
    "# Output:\n",
    "#     Model Report\n",
    "#     Accuracy : 0.8092\n",
    "#     AUC Score (Train): 0.8266\n",
    "#     Best Iteration: 1602\n",
    "#     Best ntree limit: 1603\n",
    "#     --- 2614.470659017563 seconds ---\n",
    "# So, using this slower learning rate, we have found that the optimum n_estimators value is 1603,\n",
    "# which yields an AUC value of 0.8266."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The final Feature Importances bar chart can be viewed [here](https://github.com/DavidSchanzer823239622/UTS_ML2019_82329622/blob/master/Feature%20Importances%20-%20final.png \"Final Feature Importances bar chart\"))**\n",
    "\n",
    "The top 5 features on the above bar chart are:\n",
    "1. Distance from residential address to location of most recent episode >= 11 kms\n",
    "2. Age at most recent episode >= 71 years\n",
    "3. Age at most recent episode <= 56 years\n",
    "4. Number of days the woman had to wait for results after her most recent attendance >= 9 days\n",
    "5. Total number of episodes >= 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.65, gamma=0,\n",
      "              learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=4, missing=None, n_estimators=1603, n_jobs=1,\n",
      "              nthread=4, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=7, reg_lambda=1, scale_pos_weight=1, seed=27,\n",
      "              silent=None, subsample=0.95, verbosity=1)\n",
      "Tuned accuracy: 79.53%\n",
      "--- 477.62118005752563 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Evaluation: call XGBClassifier with the optimised parameters to measure the final achieved accuracy\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model to training data\n",
    "model_final = \\\n",
    "    xgb.XGBClassifier(learning_rate = 0.01, n_estimators = 1603, max_depth = 6, min_child_weight = 4, gamma = 0,\n",
    "                      subsample = 0.95, colsample_bytree = 0.65, reg_alpha = 7, objective = 'binary:logistic',\n",
    "                      nthread = 4, scale_pos_weight = 1, seed = 27)\n",
    "model_final.fit(X_train, y_train)\n",
    "print(model_final)\n",
    "\n",
    "# Make predictions for the test data\n",
    "y_pred = model_final.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate the predictions made using the test data\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Tuned accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "dump(model_final, 'model_final.joblib')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Output:\n",
    "#     XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#                   colsample_bynode=1, colsample_bytree=0.65, gamma=0,\n",
    "#                   learning_rate=0.01, max_delta_step=0, max_depth=6,\n",
    "#                   min_child_weight=4, missing=None, n_estimators=1603, n_jobs=1,\n",
    "#                   nthread=4, objective='binary:logistic', random_state=0,\n",
    "#                   reg_alpha=7, reg_lambda=1, scale_pos_weight=1, seed=27,\n",
    "#                   silent=None, subsample=0.95, verbosity=1)\n",
    "#     Tuned accuracy: 79.53%\n",
    "#     --- 477.62118005752563 seconds ---\n",
    "# So, the final accuracy on the test dataset is 79.53%, compared to the baseline accuracy of 79.00%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Execution on Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the execution of individual Jupyter Notebook code cells is shown above as comments at the end of each cell. As explained in the Challenges section above, these results were for the 100,000 randomly-chosen observations out of the total available 824,812 observations in the full data set, and this was done for performance reasons, due to the excessive execution times encountered when attempting to use the full data set.\n",
    "\n",
    "The sample data that has been uploaded to the GitHub folder (https://github.com/DavidSchanzer823239622/UTS_ML2019_82329622), in file \"Data extraction - 1000 observations scrambled.csv\", contains only 1000 observations whose data values have been scrambled, due to the sensitivity of the BreastScreen data. Scrambled in this context means that the data has been imported into Microsoft Excel and then each column has been sorted independently of all other columns, thus creating a sample data set that has a real appearance but cannot be reidentified to an individual. As a result, when running this Notebook in Google Colab, different results will be seen, and this is why the execution results from the 100,000 observations have been included in the comments, to aid the marker of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a table showing the effect on **accuracy and AUC** of the 5 different approaches to XGBoost model training and tuning:\n",
    "\n",
    "| Approach | Description | Accuracy | AUC |\n",
    "| --: | :--------------------- | :-------------: | :----------------------------------------------------------- |\n",
    "|   1 | Call XGBClassifier with no parameters, using all of the default values, to measure the baseline accuracy | 79.00% | N/A |\n",
    "|   2 | Utilise XGBoost's built-in cross-validation capabilities | 78.85% | 0.7878 |\n",
    "|   3 | Utilise a higher number of boosting rounds with automated boosting round selection using early_stopping | 79.10% | 0.7943 |\n",
    "|   4 | Tuning a few of the above hyperparameters using a very simple GridSearch | N/A | 0.7955 |\n",
    "|   5 | Try a step-by-step approach to hyperparameter tuning: |  |  |\n",
    "|   5.1 | Fix learning_rate and n_estimators with typical values for other parameters | 80.53% | 0.8213 |\n",
    "|   5.2 | Tune max_depth and min_child_weight using wide ranges | N/A | 0.7984 |\n",
    "|   5.3 | Fine-tune max_depth and min_child_weight looking for optimum values | N/A | 0.7989 |\n",
    "|   5.4 | Tune gamma using the parameters already tuned | N/A | 0.7989 |\n",
    "|   5.5 | Re-calibrate the number of boosting rounds for the updated parameters | 81.14% | 0.8316 |\n",
    "|   5.6 | Tune subsample and colsample_bytree using wide values | N/A | 0.7991 |\n",
    "|   5.7 | Fine-tune subsample and colsample_bytree looking for optimum values | N/A | 0.7995 |\n",
    "|   5.8 | Apply regularization to reduce overfitting, by tuning reg_alpha | N/A | 0.7997 |\n",
    "|   5.9 | Fine-tune reg_alpha looking for optimum values | N/A | 0.8004 |\n",
    "|   5.10 | Apply this regularization (reg_alpha) in the model and look at the impact | 81.14% | 0.8319 |\n",
    "|   5.11 | Finally, lower the learning rate and add more trees, using the cv function of XGBoost | 80.92% | 0.8266 |\n",
    "| Evaluation | Compare the final accuracy to the baseline accuracy | 79.53% | N/A |\n",
    "\n",
    "As can be seen, Accuracy was the measure used to evaluate the ultimate effect of the tuning effort, with AUC used as the measure to be optimised during the hyperparameter tuning effort itself, especially with the step-by-step hyperparameter tuning undertaken during Approach 5.\n",
    "\n",
    "In the end, the accuracy of the model could only be increased from 79.00% (baseline) to 79.53% (final). This could be due to a number of factors:\n",
    "1. The use of a subset of 100,000 observations of the full data set only which, although randomly sampled, may have skewed training and testing sets for the model.\n",
    "2. A lack of time to perform more extensive hyperparameter tuning, such as additional regularization using reg_lambda.\n",
    "3. The 70/30 split chosen for the training and testing sets; alternative splits may result in differing model accuracy.\n",
    "4. The default values for XGBoost parameters may already be quite good default values, and therefore good accuracy could be obtained using them without any hyperparameter tuning.\n",
    "5. Other factors unknown to me due to my lack of experience with Machine Learning in general and XGBoost in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a table showing the **execution duration (elapsed time)** of the 5 different approaches to XGBoost model training and tuning:\n",
    "\n",
    "| Approach | Description | Execution Time (secs) |\n",
    "| --: | :--------------------- | --------------: |\n",
    "|   1 | Call XGBClassifier with no parameters, using all of the default values, to measure the baseline accuracy | **91** |\n",
    "|   2 | Utilise XGBoost's built-in cross-validation capabilities | **43** |\n",
    "|   3 | Utilise a higher number of boosting rounds with automated boosting round selection using early_stopping | **137** |\n",
    "|   4 | Tuning a few of the above hyperparameters using a very simple GridSearch | **155** |\n",
    "|   5 | Try a step-by-step approach to hyperparameter tuning: | **13559** |\n",
    "|   5.1 | Fix learning_rate and n_estimators with typical values for other parameters | 266 |\n",
    "|   5.2 | Tune max_depth and min_child_weight using wide ranges | 1784 |\n",
    "|   5.3 | Fine-tune max_depth and min_child_weight looking for optimum values | 1473 |\n",
    "|   5.4 | Tune gamma using the parameters already tuned | 752 |\n",
    "|   5.5 | Re-calibrate the number of boosting rounds for the updated parameters | 234 |\n",
    "|   5.6 | Tune subsample and colsample_bytree using wide values | 2208 |\n",
    "|   5.7 | Fine-tune subsample and colsample_bytree looking for optimum values | 1309 |\n",
    "|   5.8 | Apply regularization to reduce overfitting, by tuning reg_alpha | 705 |\n",
    "|   5.9 | Fine-tune reg_alpha looking for optimum values | 1796 |\n",
    "|   5.10 | Apply this regularization (reg_alpha) in the model and look at the impact | 418 |\n",
    "|   5.11 | Finally, lower the learning rate and add more trees, using the cv function of XGBoost | 2614 |\n",
    "| Evaluation | Compare the final accuracy to the baseline accuracy | 478 |\n",
    "\n",
    "An analysis of the efficiency of the above approach, when also taking into account the Accuracy test results from the previous section, reveals that:\n",
    "1. The baseline accuracy, achieved with a mere 91 seconds of execution time, achieved a level of accuracy that proved surprisingly difficult to improve upon to any great degree.\n",
    "2. The efficiency of Approaches 2 to 4, which used simple cross-validation and very elementary Grid Searching, was good (low execution time) but had little effect on accuracy.\n",
    "3. Approach 5, while highly methodical, required 13,559 seconds (3 hrs 46 mins) of execution time while only increasing accuracy by a little over 0.5%, and so can be considered to be an inefficient process. However, the effort may be considered by the business to be worthwhile to increase accuracy, but this highlights the diminishing returns that can be achieved through excessive hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for Future Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ways in which model accuracy may be further improved are:\n",
    "1. Repeating the analysis using the full data set (ie. all 824,812 observations).\n",
    "2. Additional feature engineering (creating additional features from existing features), as well as locating other possibly correlated features from the original data source.\n",
    "3. Perform finer-level binning of numerical features, which are currently only binned into 5 quantiles.\n",
    "4. Perform Principal Components Analysis on the data set to determine correlated components, to reduce the number of dimensions used to train the model. For instance, the Country of Birth and Main Language Spoken attributes had high cardinality and therefore generated many columns in the one-hot encoding step, but did not appear in the Feature Importances bar chart and therefore can be considered to have very low predictive correlation with the target variable. Removing these may also reduce run times and therefore improve efficiency.\n",
    "5. Measure the effect of different train/test splits (other than the 70/30 split used here) on model accuracy.\n",
    "6. Measure the effect of tuning other XGBoost parameters, such as reg_lambda, on model accuracy.\n",
    "7. Perform a comparative study utilising other classifiers such as Random Forest or Neural Networks. These were not undertaken as part of this study due to the additional complexity that would be added to this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering the ethical aspects of the above study, it seems appropriate to adopt a Deontological (duty-based)  approach to ethics. This is because the purpose of the study is to raise rescreen rates for BreastScreen NSW, with the aim of detecting breast cancer earlier in its development when it is more responsive to treatment, and under a Deontological ethical framework, it is one's duty to prevent suffering if it is within one's power to do so, simply because it is \"the right thing to do\".\n",
    "\n",
    "Deontologists would agree that this is true even if it sometimes produces a \"bad result\". In the case of this study, which seeks to accurately predict whether each woman in the screening program will be \"regular\" or \"lapsed\" at her next appointment, a \"bad result\" can be considered to be a false prediction. Specifically, there are two possibilities for a false prediction:\n",
    "1. A woman may be predicted to be lapsed (and therefore receives an individual-level intervention such as a phone call) but in fact presents for breast screening within the \"regular\" period of 90 days from the rescreen date.\n",
    "2. Conversely, a woman may be predicted to be \"regular\" but in fact either presents later than 90 days from the rescreen date, or not at all.\n",
    "\n",
    "To begin with the first case (ie. a false positive), we can say that either the woman's \"regular\" presentation was specifically **in response to** the intervention, or that she would have presented within 90 days in the absence of the intervention. In the former case, there is no \"bad result\" because the intervention resulted in her on-time attendance and therefore it reduced her risk of undetected breast cancer. In the latter case, she received an unnecessary phone call which, while it may have been annoying and unnecessary for her, can be justified under the Deontological framework because it was still the \"right thing\" to intervene because of her wrongly-predicted risk of  being \"lapsed\".\n",
    "\n",
    "To consider the second case above, namely a false negative where a woman may be predicted to be \"regular\" but in fact either presents later than 90 days from the rescreen date or not at all, we can say that the failure of the model to predict her \"lapsedness\" has failed to reduce her risk of undetected breast cancer. However, it also has not increased it, as the consequence is that an intervention was not undertaken for her, and therefore her risk is the same as in the absence of the deployed prediction model. Deontologists would state that the absence of 100% accuracy in the prediction model is not a reason to avoid using it, since some \"good\" is still being done to the majority of women, despite the absence of \"good\" being done to a minority.\n",
    "\n",
    "It is difficult to anticipate potential misuses of the technique presented in this study, with the possible exception of an extremely poor prediction model being deployed into production that would result in a large number of unneeded interventions in the case of the false postives, and a large number of missed interventions in the case of the false negatives. This misuse could be prevented were the business (BreastScreen NSW) to set a minimum level of prediction accuracy required before a solution is deployed into production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project set out to determine the accuracy with which the attendance of women at their next breast screening appointment at a BreastScreen NSW clinic or mobile van can be predicted, classifying women as either \"regular\" (predicted to attend within 90 days of the rescreen date) or \"lapsed\" (predicted to not attend within 90 days of their rescreen date). Within the parameters of this project, an accuracy of 79.53% has been achieved, with a number of suggestions for how this might be improved.\n",
    "\n",
    "This provides BreastScreen NSW with the opportunity to consider whether this level of prediction accuracy is sufficient to be deployed to allow individual-level interventions (eg. a reminder phone call) to raise resceeen rates for the estimated 37% of NSW women who are either under-screened or have lapsed from the program completely.\n",
    "\n",
    "The 5 key predictors of non-reattendance at BreastScreen NSW have been identified to be:\n",
    "1. Distance from residential address to location of most recent episode >= 11 kms\n",
    "2. Age at most recent episode >= 71 years\n",
    "3. Age at most recent episode <= 56 years\n",
    "4. Number of days the woman had to wait for results after her most recent attendance >= 9 days\n",
    "5. Total number of episodes >= 10 episodes\n",
    "\n",
    "Most of these predictor factors are outside of BreastScreen NSW's control. However, these results indicate that any changes that can be made to operational procedures in order to reduce the length of time that women wait for results may increase rescreening rates.\n",
    "\n",
    "A more rigorous study, utilising additional classifiers and Machine Learning expertise in addition to the suggestions  above, may well yield improved model accuracy and therefore result in a more deployment-ready \"regular\" vs. \"lapsed\" prediction tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1 - SQL for Data Acquisition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "USE tempdb;\n",
    "GO\n",
    "\n",
    "DROP TABLE IF EXISTS dbo.LapsedRegularPrediction;\n",
    "GO\n",
    "\n",
    "CREATE TABLE dbo.LapsedRegularPrediction\n",
    "(\n",
    "    CountryOfBirth\t\t\t\t\t\t\tVARCHAR(250) NOT NULL,\n",
    "    MainLanguage\t\t\t\t\t\t\tVARCHAR(250) NOT NULL,\n",
    "    IndigenousStatus\t\t\t\t\t\tVARCHAR(250) NOT NULL,\n",
    "    RemotenessArea\t\t\t\t\t\t\tVARCHAR(100) NULL,\n",
    "    RelativeSocioEconomicDisadvantageDecile\tINT NULL,\n",
    "    HistoryPreviousCancer\t\t\t\t\tINT NOT NULL,\n",
    "    HistoryFamilyCancer\t\t\t\t\t\tINT NOT NULL,\n",
    "    TotalEpisodes\t\t\t\t\t\t\tINT NULL,\n",
    "    HasBeenDNA\t\t\t\t\t\t\t\tINT NOT NULL,\n",
    "    HadAssessment\t\t\t\t\t\t\tINT NOT NULL,\n",
    "    HadNeedleBiopsy\t\t\t\t\t\t\tINT NOT NULL,\n",
    "    HadTechRecall\t\t\t\t\t\t\tINT NOT NULL,\n",
    "    AgeAtMostRecentEpisode\t\t\t\t\tINT NULL,\n",
    "    DistanceKms\t\t\t\t\t\t\t\tINT NULL,\n",
    "    MonthMostRecentScreening\t\t\t\tINT NULL,\n",
    "    DayOfWeekMostRecentScreening\t\t\tINT NULL,\n",
    "    HourOfDayMostRecentScreening\t\t\tINT NULL,\n",
    "    VenueTypeMostRecentScreening\t\t\tVARCHAR(250) NULL,\n",
    "    FilmsTakenMostRecentScreening\t\t\tINT NULL,\n",
    "    DaysFromAttendanceToResultSent\t\t\tINT NULL,\n",
    "    LapsedRegular3rdMostRecentEpisode\t\tVARCHAR(7) NULL,\n",
    "    LapsedRegular2ndMostRecentEpisode\t\tVARCHAR(7) NULL,\n",
    "    LapsedRegularMostRecentEpisode\t\t\tVARCHAR(7) NULL\n",
    ");\n",
    "GO\n",
    "\n",
    "USE DATA_MART_SCREENING;\n",
    "GO\n",
    "\n",
    "CREATE NONCLUSTERED INDEX IDX_DIM_DNA_STATUS_HISTORY_APPOINTMENT_ID_DD ON dbo.DIM_DNA_STATUS_HISTORY (APPOINTMENT_ID_DD);\n",
    "GO\n",
    "\n",
    "INSERT INTO tempdb.dbo.LapsedRegularPrediction\n",
    "\tSELECT \n",
    "\t\t   -- Country of birth. 215 distinct values: 65.8% = 'AUSTRALIA', 5.9% = 'UNITED KINGDOM', 2.4% = 'CHINA', etc.\n",
    "\t\t   COUNTRY_OF_BIRTH_DESCRIPTION AS CountryOfBirth,\n",
    "\t\t   -- Main language spoken. 22 distinct values: 81.3% = 'English Only', 3.8% = 'Other (please specify)', 2.4% = 'Cantonese', etc.\n",
    "\t\t   MAIN_LANGUAGE_DESCRIPTION AS MainLanguage,\n",
    "\t\t   -- Indigenous status. 6 distinct values: 98.4% = 'Non-indigenous', 1.2% = 'Aboriginal', 0.27% = 'Not stated', 0.06% = 'Aboriginal and Torres Strait Islander', 0.03% = 'Torres Strait Islander', 0.02% = 'Declines to Respond'\n",
    "\t\t   INDIGENOUS_STATUS_DESCRIPTION AS IndigenousStatus,\n",
    "\t\t   -- Remoteness Area.  6 distinct values: 0.97% = NULL, then 69.7% = 'Major Cities of Australia', 23.6% = 'Inner Regional Australia', 5.4% = 'Outer Regional Australia', 0.33% = 'Remote Australia', 0.07% = 'Very Remote Australia'\n",
    "\t\t   ISNULL(( SELECT RA_NAME_2016 FROM tempdb.dbo.MappingLGAToRAReduced AS T WHERE T.LGA_CODE = DCD.LGA_CODE OR T.LGA_NAME = DCD.LOCAL_GOVERNMENT_AREA ), '') AS RemotenessArea,\n",
    "\t\t\t-- Relative Socio-economic Disadvantage Decile: 2.6% = NULL, then 28.1% = 10, 17.8% = 6, 9.8% = 3, 9.5% = 5, etc.\n",
    "\t\t   ISNULL(( SELECT RelativeSocioEconomicDisadvantageDecile FROM tempdb.dbo.MappingLGAToSEIFA AS T WHERE T.LGA_CODE = DCD.LGA_CODE OR T.LGA_NAME = DCD.LOCAL_GOVERNMENT_AREA ), '') AS RelativeSocioEconomicDisadvantageDecile,\n",
    "\t\t   -- Does the woman have any history of previous cancer external to program? 2 distinct values: 98.6% = 0, 1.4% = 1\n",
    "\t\t   CASE ( SELECT DISTINCT 1 FROM DATA_MART_SCREENING.dbo.FACT_ATTENDANCE_PREVIOUS_CANCER AS FAPC WHERE FAPC.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK ) WHEN 1 THEN 1 ELSE 0 END AS HistoryPreviousCancer,\n",
    "\t\t   -- Does the woman have any family history of cancer? 2 distinct values: 74.6% = 0, 25.4% = 1\n",
    "\t\t   CASE ( SELECT DISTINCT 1 FROM DATA_MART_SCREENING.dbo.FACT_ATTENDANCE_FAMILY_HISTORY AS FAFH WHERE FAFH.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK ) WHEN 1 THEN 1 ELSE 0 END AS HistoryFamilyCancer,\n",
    "\t\t   -- Total number of episodes: min = 2, max = 29, avg = 6, stdev = 3.8\n",
    "\t\t   ( SELECT COUNT(*) FROM DATA_MART_SCREENING.dbo.FACT_EPISODE AS FE WHERE FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK ) AS TotalEpisodes,\n",
    "\t\t   -- Has the woman been \"DNA\" (did not attend) at any point in her past as part of the program?: 72.7% = 0, 27.3% = 1\n",
    "\t\t   CASE ( SELECT DISTINCT 1 FROM DATA_MART_SCREENING.dbo.FACT_APPOINTMENT AS FA WHERE FA.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK AND (( FA.DIM_APPOINTMENT_STATUS_SK = 28 /* = 'Confirmed' */ AND FA.DIM_LAST_DNA_ACTION_DATE_SK < GETDATE()) /* Client is DNA right now */ OR ( FA.DIM_APPOINTMENT_STATUS_SK IN (30, 31) /* = 'Cancelled by Programme' or 'Cancelled by Client' */ AND ( SELECT COUNT(*) from dbo.DIM_DNA_STATUS_HISTORY AS DDSH WHERE DDSH.APPOINTMENT_ID_DD = FA.APPOINTMENT_ID_DD ) > 0 ))) WHEN 1 THEN 1 ELSE 0 END AS HasBeenDNA,\n",
    "\t\t   -- Has the woman had an assessment at any point in her past as part of the program? 2 values: 74.6% = 0, 25.4% = 1\n",
    "\t\t   CASE ( SELECT DISTINCT 1 FROM DATA_MART_SCREENING.dbo.FACT_ATTENDANCE_ASSESSMENT AS FAA WHERE FAA.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK ) WHEN 1 THEN 1 ELSE 0 END AS HadAssessment,\n",
    "\t\t   -- Has the woman had a needle biopsy at any point in her past as part of the program? 2 values: 93.4% = 0, 6.6% = 1\n",
    "\t\t   CASE ( SELECT DISTINCT 1 FROM DATA_MART_SCREENING.dbo.FACT_NEEDLE_BIOPSY AS FNB WHERE FNB.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK ) WHEN 1 THEN 1 ELSE 0 END AS HadNeedleBiopsy,\n",
    "\t\t   -- Did the woman have a Technical Recall at any point in her past as part of the program? 2 values: 96.9% = 0, 3.1% = 1\n",
    "\t\t   CASE ( SELECT DISTINCT 1 FROM DATA_MART_SCREENING.dbo.FACT_ATTENDANCE_SCREENING AS FAS INNER JOIN DATA_MART_SCREENING.dbo.DIM_LK_Visit_Type AS DLVT ON FAS.DIM_ATTENDANCE_VISIT_TYPE_SK = DLVT.DIM_LK_Visit_Type_SK AND DLVT.DESCRIPTION = 'Technical recall' WHERE FAS.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK ) WHEN 1 THEN 1 ELSE 0 END AS HadTechRecall,\n",
    "\t\t   -- Age at most recent episode: NULL = 0%, min = 40, max = 109, avg = 63, stdev = 8.3\n",
    "\t\t   ( SELECT MAX(FE.DIM_CLIENT_AGE_AT_EPISODE_START_DATE_SK) FROM FACT_EPISODE AS FE WHERE FE.DIM_CLIENT_AGE_AT_EPISODE_START_DATE_SK >= 16 AND FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK AND FE.DIM_EPISODE_START_DATE_SK = ( SELECT MAX(DIM_EPISODE_START_DATE_SK) FROM FACT_EPISODE AS FE2 WHERE FE2.DIM_CLIENT_SK = FE.DIM_CLIENT_SK)) AS AgeAtMostRecentEpisode,\n",
    "\t\t   -- Distance from residential address to location of most recent episode: 1.23% = NULL, min = 0, max = 3860, avg = 15.6, stdev = 82.6\n",
    "\t\t   ISNULL(( SELECT TOP 1 CASE WHEN DCD.LATITUDE IS NOT NULL AND DCD.LONGITUDE IS NOT NULL AND DCD.LATITUDE <> 0 AND DCD.LONGITUDE <> 0 AND COALESCE( VGI1.LATITUDE, VGI2.LATITUDE) IS NOT NULL AND COALESCE( VGI1.LONGITUDE, VGI2.LONGITUDE) IS NOT NULL THEN geography::Point(CAST(DCD.LATITUDE AS VARCHAR(30)), CAST(DCD.LONGITUDE AS VARCHAR(30)), 4326).STDistance(geography::Point(CAST(COALESCE( VGI1.LATITUDE, VGI2.LATITUDE) AS VARCHAR(30)), CAST(COALESCE( VGI1.LONGITUDE, VGI2.LONGITUDE) AS VARCHAR(30)), 4326)) / 1000.0 ELSE NULL END FROM dbo.FACT_EPISODE AS FE LEFT OUTER JOIN dbo.FACT_ATTENDANCE_SCREENING AS FAS ON FAS.EPISODE_ID_DD = FE.EPISODE_ID_DD AND FAS.ATTENDANCE_NUMBER = 1 LEFT OUTER JOIN dbo.FACT_ATTENDANCE_ASSESSMENT AS FAA ON FAA.EPISODE_ID_DD = FE.EPISODE_ID_DD AND FAA.ATTENDANCE_NUMBER = 1 LEFT OUTER JOIN tempdb.dbo.VenueGeocodesIntech AS VGI1 ON VGI1.VENUE_SK = FAS.DIM_VENUE_SK LEFT OUTER JOIN tempdb.dbo.VenueGeocodesIntech AS VGI2 ON VGI2.VENUE_SK = FAA.DIM_VENUE_SK WHERE FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK AND FE.DIM_EPISODE_START_DATE_SK = ( SELECT MAX(FE2.DIM_EPISODE_START_DATE_SK) FROM dbo.FACT_EPISODE AS FE2 WHERE FE2.DIM_CLIENT_SK = FE.DIM_CLIENT_SK ) AND FE.DIM_LAST_UPDATED_DATE_SK = ( SELECT MAX(FE2.DIM_LAST_UPDATED_DATE_SK) FROM dbo.FACT_EPISODE AS FE2 WHERE FE2.DIM_CLIENT_SK = FE.DIM_CLIENT_SK ) AND FE.EPISODE_NUMBER = ( SELECT MAX(FE2.EPISODE_NUMBER) FROM dbo.FACT_EPISODE AS FE2 WHERE FE2.DIM_CLIENT_SK = FE.DIM_CLIENT_SK )), '') AS DistanceKms,\n",
    "\t\t   -- Month of year of most recent screening attendance. 13 values: 0.006% = NULL, 10.0% = 8 (May), ..., 4.8% = 12 (Dec)\n",
    "\t\t   ISNULL(( SELECT DISTINCT DATEPART(MONTH, FAS.DIM_DATE_OF_ARRIVAL_SK) FROM dbo.FACT_ATTENDANCE_SCREENING AS FAS WHERE FAS.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK AND FAS.DIM_DATE_OF_ARRIVAL_SK = ( SELECT MAX(FAS2.DIM_DATE_OF_ARRIVAL_SK) FROM dbo.FACT_ATTENDANCE_SCREENING AS FAS2 WHERE FAS2.DIM_CLIENT_SK = FAS.DIM_CLIENT_SK )), '') AS MonthMostRecentScreening,\n",
    "\t\t   -- Day of week of most recent screening attendance. 8 values: 0.006% = NULL, 21.8% = 4 (Wed), ..., 0.54% = 1 (Sun)\n",
    "\t\t   ISNULL(( SELECT DISTINCT DATEPART(WEEKDAY, FAS.DIM_DATE_OF_ARRIVAL_SK) FROM dbo.FACT_ATTENDANCE_SCREENING AS FAS WHERE FAS.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK AND FAS.DIM_DATE_OF_ARRIVAL_SK = ( SELECT MAX(FAS2.DIM_DATE_OF_ARRIVAL_SK) FROM dbo.FACT_ATTENDANCE_SCREENING AS FAS2 WHERE FAS2.DIM_CLIENT_SK = FAS.DIM_CLIENT_SK )), '') AS DayOfWeekMostRecentScreening,\n",
    "\t\t   -- Hour of day of most recent screening attendance. 25 values: 0.006% = NULL, 15.0% = 11 (11am-12pm), 14.3% = 9 (9am-10pm), ..., 0.0001% = 21 (9-10pm)\n",
    "\t\t   ISNULL(( SELECT MAX(DATEPART(HOUR, FAS.DIM_TIME_OF_ARRIVAL_SK)) FROM dbo.FACT_ATTENDANCE_SCREENING AS FAS WHERE FAS.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK AND FAS.DIM_DATE_OF_ARRIVAL_SK = ( SELECT MAX(FAS2.DIM_DATE_OF_ARRIVAL_SK) FROM dbo.FACT_ATTENDANCE_SCREENING AS FAS2 WHERE FAS2.DIM_CLIENT_SK = FAS.DIM_CLIENT_SK )), '') AS HourOfDayMostRecentScreening,\n",
    "\t\t   -- Type of venue of most recent screening attendance. 3 values: 11.3% = NULL, 66.7% = 'Fixed', 22.0% = 'Mobile'\n",
    "\t\t   ISNULL(( SELECT MAX(DSS.UNIT_TYPE_DESCRIPTION) FROM dbo.FACT_EPISODE AS FE LEFT OUTER JOIN dbo.FACT_ATTENDANCE_SCREENING AS FAS ON FAS.EPISODE_ID_DD = FE.EPISODE_ID_DD AND FAS.ATTENDANCE_NUMBER = 1 LEFT OUTER JOIN dbo.DIM_VENUE AS DV ON DV.DIM_VENUE_SK = FAS.DIM_VENUE_SK AND FAS.DIM_APPOINTMENT_DATE_SK BETWEEN DV.VALID_FROM_DATE AND DV.VALID_TO_DATE LEFT OUTER JOIN dbo.DIM_SCREENING_SITE AS DSS ON DSS.DIM_SCREENING_SITE_SK = DV.SCREENING_SITE_CODE WHERE FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK AND FE.DIM_EPISODE_START_DATE_SK = ( SELECT MAX(FE2.DIM_EPISODE_START_DATE_SK) FROM dbo.FACT_EPISODE AS FE2 WHERE FE2.DIM_CLIENT_SK = FE.DIM_CLIENT_SK )), '') AS VenueTypeMostRecentScreening,\n",
    "\t\t   -- Number of films taken at most recent screening attendance: 0.07% = NULL, min = 0, max = 54, avg = 4, stdev = 0.87\n",
    "\t\t   ISNULL(( SELECT SUM(FM.NUMBER_OF_FILMS_TAKEN) FROM DATA_MART_SCREENING.dbo.FACT_EPISODE AS FE INNER JOIN dbo.FACT_MAMMOGRAM AS FM ON FM.EPISODE_ID_DD = FE.EPISODE_ID_DD WHERE FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK AND FE.DIM_EPISODE_START_DATE_SK = ( SELECT MAX(FE2.DIM_EPISODE_START_DATE_SK) FROM dbo.FACT_EPISODE AS FE2 WHERE FE2.DIM_CLIENT_SK = FE.DIM_CLIENT_SK )), '') AS FilmsTakenMostRecentScreening,\n",
    "\t\t   -- How many days did the woman have to wait for results after her most recent attendance?: 0.82% = NULL, min = 0, max = 100, avg = 6, stdev = 4.6\n",
    "\t\t   ISNULL(( SELECT TOP 1 CASE WHEN DATEDIFF(DAY, CASE WHEN FAS.DIM_DATE_OF_ARRIVAL_SK IS NULL THEN FAA.DIM_DATE_OF_ARRIVAL_SK WHEN FAA.DIM_DATE_OF_ARRIVAL_SK IS NULL THEN FAS.DIM_DATE_OF_ARRIVAL_SK WHEN FAA.DIM_DATE_OF_ARRIVAL_SK > FAS.DIM_DATE_OF_ARRIVAL_SK THEN FAA.DIM_DATE_OF_ARRIVAL_SK ELSE FAS.DIM_DATE_OF_ARRIVAL_SK END, CASE WHEN FAS.DIM_DATE_OF_ARRIVAL_SK IS NULL THEN (SELECT MAX(DIM_NOTIFICATION_SENT_DATE_SK) FROM dbo.FACT_NOTIFICATION_CLIENT AS FNC WHERE FNC.DIM_CLIENT_SK = FE.DIM_CLIENT_SK AND FNC.ATTENDANCE_ID_DD = FAA.ATTENDANCE_ID_DD) WHEN FAA.DIM_DATE_OF_ARRIVAL_SK IS NULL THEN (SELECT MAX(DIM_NOTIFICATION_SENT_DATE_SK) FROM dbo.FACT_NOTIFICATION_CLIENT AS FNC WHERE FNC.DIM_CLIENT_SK = FE.DIM_CLIENT_SK AND FNC.ATTENDANCE_ID_DD = FAS.ATTENDANCE_ID_DD) WHEN FAA.DIM_DATE_OF_ARRIVAL_SK > FAS.DIM_DATE_OF_ARRIVAL_SK THEN (SELECT MAX(DIM_NOTIFICATION_SENT_DATE_SK) FROM dbo.FACT_NOTIFICATION_CLIENT AS FNC WHERE FNC.DIM_CLIENT_SK = FE.DIM_CLIENT_SK AND FNC.ATTENDANCE_ID_DD = FAA.ATTENDANCE_ID_DD) ELSE (SELECT MAX(DIM_NOTIFICATION_SENT_DATE_SK) FROM dbo.FACT_NOTIFICATION_CLIENT AS FNC WHERE FNC.DIM_CLIENT_SK = FE.DIM_CLIENT_SK AND FNC.ATTENDANCE_ID_DD = FAS.ATTENDANCE_ID_DD) END) BETWEEN 0 AND 100 THEN DATEDIFF(DAY, CASE WHEN FAS.DIM_DATE_OF_ARRIVAL_SK IS NULL THEN FAA.DIM_DATE_OF_ARRIVAL_SK WHEN FAA.DIM_DATE_OF_ARRIVAL_SK IS NULL THEN FAS.DIM_DATE_OF_ARRIVAL_SK WHEN FAA.DIM_DATE_OF_ARRIVAL_SK > FAS.DIM_DATE_OF_ARRIVAL_SK THEN FAA.DIM_DATE_OF_ARRIVAL_SK ELSE FAS.DIM_DATE_OF_ARRIVAL_SK END, CASE WHEN FAS.DIM_DATE_OF_ARRIVAL_SK IS NULL THEN (SELECT MAX(DIM_NOTIFICATION_SENT_DATE_SK) FROM dbo.FACT_NOTIFICATION_CLIENT AS FNC WHERE FNC.DIM_CLIENT_SK = FE.DIM_CLIENT_SK AND FNC.ATTENDANCE_ID_DD = FAA.ATTENDANCE_ID_DD) WHEN FAA.DIM_DATE_OF_ARRIVAL_SK IS NULL THEN (SELECT MAX(DIM_NOTIFICATION_SENT_DATE_SK) FROM dbo.FACT_NOTIFICATION_CLIENT AS FNC WHERE FNC.DIM_CLIENT_SK = FE.DIM_CLIENT_SK AND FNC.ATTENDANCE_ID_DD = FAS.ATTENDANCE_ID_DD) WHEN FAA.DIM_DATE_OF_ARRIVAL_SK > FAS.DIM_DATE_OF_ARRIVAL_SK THEN (SELECT MAX(DIM_NOTIFICATION_SENT_DATE_SK) FROM dbo.FACT_NOTIFICATION_CLIENT AS FNC WHERE FNC.DIM_CLIENT_SK = FE.DIM_CLIENT_SK AND FNC.ATTENDANCE_ID_DD = FAA.ATTENDANCE_ID_DD) ELSE (SELECT MAX(DIM_NOTIFICATION_SENT_DATE_SK) FROM dbo.FACT_NOTIFICATION_CLIENT AS FNC WHERE FNC.DIM_CLIENT_SK = FE.DIM_CLIENT_SK AND FNC.ATTENDANCE_ID_DD = FAS.ATTENDANCE_ID_DD) END) ELSE NULL END AS DaysFromAttendanceToResultSent FROM dbo.FACT_EPISODE AS FE LEFT OUTER JOIN dbo.FACT_ATTENDANCE_SCREENING AS FAS ON FAS.EPISODE_ID_DD = FE.EPISODE_ID_DD AND FAS.DIM_DATE_OF_ARRIVAL_SK = ( SELECT MAX(FAS2.DIM_DATE_OF_ARRIVAL_SK) FROM dbo.FACT_ATTENDANCE_SCREENING AS FAS2 WHERE FAS2.EPISODE_ID_DD = FAS.EPISODE_ID_DD ) LEFT OUTER JOIN dbo.FACT_ATTENDANCE_ASSESSMENT AS FAA ON FAA.EPISODE_ID_DD = FE.EPISODE_ID_DD AND FAA.DIM_DATE_OF_ARRIVAL_SK = ( SELECT MAX(FAA2.DIM_DATE_OF_ARRIVAL_SK) FROM dbo.FACT_ATTENDANCE_ASSESSMENT AS FAA2 WHERE FAA2.EPISODE_ID_DD = FAA.EPISODE_ID_DD ) WHERE FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK AND FE.DIM_EPISODE_START_DATE_SK = ( SELECT MAX(FE2.DIM_EPISODE_START_DATE_SK) FROM dbo.FACT_EPISODE AS FE2 WHERE FE2.DIM_CLIENT_SK = FE.DIM_CLIENT_SK )), '') AS DaysFromAttendanceToResultSent,\n",
    "\t\t   -- Was the woman 'regular' or 'lapsed' at her 3rd most recent episode? 3 values: 37.4% = NULL, 49.8% = 'Regular', 12.9% = 'Lapsed'\n",
    "\t\t   ISNULL(( SELECT LapsedOrRegular FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) AS NthMostRecentRowNumber, CASE WHEN FE.DIM_EPISODE_SCREENING_DUE_DATE_SK IS NULL OR LAG(FE.DIM_EPISODE_START_DATE_SK) OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) IS NULL OR FE.DIM_EPISODE_SCREENING_DUE_DATE_SK = '1799-01-01' THEN NULL WHEN DATEDIFF( DAY, FE.DIM_EPISODE_SCREENING_DUE_DATE_SK, LAG(FE.DIM_EPISODE_START_DATE_SK) OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) ) > 90 THEN 'Lapsed' ELSE 'Regular' END AS LapsedOrRegular FROM dbo.FACT_EPISODE AS FE WHERE FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK) AS MostRecentEpisodes WHERE NthMostRecentRowNumber = 4 ), '') AS LapsedRegular3rdMostRecentEpisode,\n",
    "\t\t   -- Was the woman 'regular' or 'lapsed' at her 2nd most recent episode? 3 values: 21.7% = NULL, 61.0% = 'Regular', 17.4% = 'Lapsed'\n",
    "\t\t   ISNULL(( SELECT LapsedOrRegular FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) AS NthMostRecentRowNumber, CASE WHEN FE.DIM_EPISODE_SCREENING_DUE_DATE_SK IS NULL OR LAG(FE.DIM_EPISODE_START_DATE_SK) OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) IS NULL OR FE.DIM_EPISODE_SCREENING_DUE_DATE_SK = '1799-01-01' THEN NULL WHEN DATEDIFF( DAY, FE.DIM_EPISODE_SCREENING_DUE_DATE_SK, LAG(FE.DIM_EPISODE_START_DATE_SK) OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) ) > 90 THEN 'Lapsed' ELSE 'Regular' END AS LapsedOrRegular FROM dbo.FACT_EPISODE AS FE WHERE FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK) AS MostRecentEpisodes WHERE NthMostRecentRowNumber = 3 ), '') AS LapsedRegular2ndMostRecentEpisode,\n",
    "\t\t   -- Was the woman 'regular' or 'lapsed' at her most recent episode? 2 values: 74.4% = 'Regular', 25.6% = 'Lapsed'\n",
    "\t\t   ( SELECT LapsedOrRegular FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) AS NthMostRecentRowNumber, CASE WHEN FE.DIM_EPISODE_SCREENING_DUE_DATE_SK IS NULL OR LAG(FE.DIM_EPISODE_START_DATE_SK) OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) IS NULL OR FE.DIM_EPISODE_SCREENING_DUE_DATE_SK = '1799-01-01' THEN NULL WHEN DATEDIFF( DAY, FE.DIM_EPISODE_SCREENING_DUE_DATE_SK, LAG(FE.DIM_EPISODE_START_DATE_SK) OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) ) > 90 THEN 'Lapsed' ELSE 'Regular' END AS LapsedOrRegular FROM dbo.FACT_EPISODE AS FE WHERE FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK) AS MostRecentEpisodes WHERE NthMostRecentRowNumber = 2 ) AS LapsedRegularMostRecentEpisode\n",
    "\tFROM DATA_MART_SCREENING.dbo.DIM_CLIENT_DEMOGRAPHIC AS DCD\n",
    "\tWHERE DCD.IS_CURRENT_YN = 'Y'\t\t\t-- Current version of each client record\n",
    "\tAND DCD.MERGED_TO_NHINUMBER IS NULL\t\t-- Has not been merged with another client\n",
    "\tAND DCD.DIM_CLIENT_SK <> -1\t\t\t\t-- Is not the \"unknown\" value\n",
    "\t\t\t\t\t\t\t\t\t\t\t-- Most recent episode could been labelled as 'lapsed' or 'regular'\n",
    "\tAND ( SELECT LapsedOrRegular FROM ( SELECT ROW_NUMBER() OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) AS NthMostRecentRowNumber, CASE WHEN FE.DIM_EPISODE_SCREENING_DUE_DATE_SK IS NULL OR LAG(FE.DIM_EPISODE_START_DATE_SK) OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) IS NULL OR FE.DIM_EPISODE_SCREENING_DUE_DATE_SK = '1799-01-01' THEN NULL WHEN DATEDIFF( DAY, FE.DIM_EPISODE_SCREENING_DUE_DATE_SK, LAG(FE.DIM_EPISODE_START_DATE_SK) OVER (PARTITION BY FE.DIM_CLIENT_SK ORDER BY FE.DIM_EPISODE_START_DATE_SK DESC) ) > 90 THEN 'Lapsed' ELSE 'Regular' END AS LapsedOrRegular FROM dbo.FACT_EPISODE AS FE WHERE FE.DIM_CLIENT_SK = DCD.DIM_CLIENT_SK) AS MostRecentEpisodes WHERE NthMostRecentRowNumber = 2 ) IS NOT NULL\n",
    ";\n",
    "GO\n",
    "\n",
    "DROP INDEX dbo.DIM_DNA_STATUS_HISTORY.IDX_DIM_DNA_STATUS_HISTORY_APPOINTMENT_ID_DD;\n",
    "GO\n",
    "\n",
    "SELECT * FROM tempdb.dbo.LapsedRegularPrediction;\n",
    "GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
